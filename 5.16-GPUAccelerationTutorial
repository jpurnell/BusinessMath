# GPU Acceleration Tutorial

**BusinessMath 2.0** - Genetic Algorithm GPU Acceleration

---

## Overview

BusinessMath's genetic algorithm optimizer provides **automatic GPU acceleration** for large-scale optimization problems, delivering **10-100× performance improvements** with zero code changes.

### Key Features

✅ **Transparent acceleration** - GPU activates automatically when beneficial
✅ **10-100× speedup** for populations ≥ 1,000 on Apple Silicon
✅ **Zero API changes** - Same code runs on CPU or GPU
✅ **Graceful fallback** - Uses CPU when Metal unavailable
✅ **Constraint support** - Equality and inequality constraints via penalty method

---

## Quick Start

### Basic Usage

```swift
import BusinessMath

// Define search space bounds
let searchSpace = [(-10.0, 10.0), (-10.0, 10.0)]

// Create optimizer (GPU activates automatically for large populations)
let optimizer = GeneticAlgorithm<VectorN<Double>>(
    config: GeneticAlgorithmConfig(
        populationSize: 2000,  // ≥ 1000 triggers GPU
        generations: 100
    ),
    searchSpace: searchSpace
)

// Minimize objective function
let sphere = { (v: VectorN<Double>) -> Double in v.dot(v) }
let result = try optimizer.minimize(sphere, from: VectorN([5.0, 5.0]))

print("Solution: \(result.solution)")     // Near [0, 0]
print("Fitness: \(result.value)")         // Near 0
print("Generations: \(result.iterations)")
```

**That's it!** GPU acceleration happens automatically when:
- Population size ≥ 1,000
- Running on macOS/iOS with Metal support
- Optimizer type is `VectorN<Double>`

---

## When GPU Accelerates

### Automatic Threshold

GPU acceleration activates based on population size:

| Population | Execution | Reason |
|------------|-----------|--------|
| < 1,000    | **CPU**   | GPU overhead dominates |
| ≥ 1,000    | **GPU**   | Parallelism amortizes overhead |

### Performance Comparison

**Benchmark: 2000 individuals × 30 generations × 10D problem (Apple M1)**

```
CPU:  ~15-30 seconds  (2,000-4,000 evals/sec)
GPU:  ~1.0 seconds    (60,000 evals/sec)

Speedup: 15-30×
```

### What Runs on GPU?

✅ **Selection** - Tournament selection parallelized
✅ **Crossover** - Uniform crossover parallelized
✅ **Mutation** - Gaussian mutation parallelized
❌ **Fitness evaluation** - Always on CPU (allows arbitrary Swift closures)

---

## Configuration Guide

### High-Performance Configuration

For large-scale problems, use the high-performance preset:

```swift
let config = GeneticAlgorithmConfig.highPerformance
// populationSize: 1000
// generations: 500
// eliteCount: 10
// tournamentSize: 5

let optimizer = GeneticAlgorithm<VectorN<Double>>(
    config: config,
    searchSpace: searchSpace
)
```

### Custom Configuration

Fine-tune parameters for your problem:

```swift
let config = GeneticAlgorithmConfig(
    populationSize: 2000,       // Larger = more exploration, slower per generation
    generations: 200,           // More = better convergence
    crossoverRate: 0.8,         // 0.7-0.9 typical (exploitation)
    mutationRate: 0.1,          // 0.01-0.2 typical (exploration)
    mutationStrength: 0.2,      // Gaussian σ relative to bounds
    eliteCount: 10,             // Preserve top N individuals
    tournamentSize: 3,          // 3-5 typical (selection pressure)
    seed: 12345                 // For reproducible results
)
```

**Parameter Trade-offs:**

- **Population Size** ↑ → Better exploration, GPU benefit ↑, time per generation ↑
- **Mutation Rate** ↑ → More exploration, slower convergence
- **Elite Count** ↑ → Faster convergence, risk of premature convergence
- **Tournament Size** ↑ → Stronger selection pressure, faster convergence

---

## Constrained Optimization

BusinessMath supports **equality and inequality constraints** via the penalty method:

### Equality Constraints

Minimize x² + y² subject to x + y = 1:

```swift
let optimizer = GeneticAlgorithm<VectorN<Double>>(
    config: GeneticAlgorithmConfig(populationSize: 1000, generations: 200),
    searchSpace: [(-5.0, 5.0), (-5.0, 5.0)]
)

let objective = { (v: VectorN<Double>) -> Double in v.dot(v) }

let constraint = MultivariateConstraint<VectorN<Double>>.equality { v in
    v[0] + v[1] - 1.0  // x + y = 1
}

let result = try optimizer.minimize(
    objective,
    from: VectorN([0.0, 0.0]),
    constraints: [constraint]
)

// Solution near (0.5, 0.5) - minimum of x²+y² on line x+y=1
print("Solution: \(result.solution)")
print("Constraint violation: \(result.solution[0] + result.solution[1] - 1.0)")
```

### Inequality Constraints

Maximize x + y subject to x² + y² ≤ 1:

```swift
let objective = { (v: VectorN<Double>) -> Double in
    -(v[0] + v[1])  // Negate for maximization
}

let constraint = MultivariateConstraint<VectorN<Double>>.inequality { v in
    v.dot(v) - 1.0  // x² + y² ≤ 1
}

let result = try optimizer.minimize(
    objective,
    from: VectorN([0.0, 0.0]),
    constraints: [constraint]
)

// Solution near (√2/2, √2/2) ≈ (0.707, 0.707)
```

### Multiple Constraints

Combine equality and inequality constraints:

```swift
let constraints = [
    MultivariateConstraint<VectorN<Double>>.inequality { v in -v[0] },        // x ≥ 0
    MultivariateConstraint<VectorN<Double>>.inequality { v in -v[1] },        // y ≥ 0
    MultivariateConstraint<VectorN<Double>>.inequality { v in 1.0 - v[0] - v[1] }  // x+y ≥ 1
]

let result = try optimizer.minimize(
    objective,
    from: VectorN([1.0, 1.0]),
    constraints: constraints
)
```

**Note:** Penalty methods provide **approximate** solutions. Violations may be small but non-zero.

---

## Real-World Examples

### Portfolio Optimization

Minimize risk subject to return constraint:

```swift
// 10 assets, minimize variance subject to 8% return
let numAssets = 10
let returns: [Double] = [0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.11, 0.12, 0.13, 0.14]
let covariance: [[Double]] = // 10×10 covariance matrix
let targetReturn = 0.08

let optimizer = GeneticAlgorithm<VectorN<Double>>(
    config: GeneticAlgorithmConfig(populationSize: 1000, generations: 200),
    searchSpace: Array(repeating: (0.0, 1.0), count: numAssets)
)

// Objective: minimize portfolio variance
let objective = { (weights: VectorN<Double>) -> Double in
    var variance = 0.0
    for i in 0..<numAssets {
        for j in 0..<numAssets {
            variance += weights[i] * weights[j] * covariance[i][j]
        }
    }
    return variance
}

// Constraints
let constraints = [
    // Weights sum to 1
    MultivariateConstraint<VectorN<Double>>.equality { w in
        w.toArray().reduce(0, +) - 1.0
    },
    // Target return ≥ 8%
    MultivariateConstraint<VectorN<Double>>.inequality { w in
        0.08 - zip(w.toArray(), returns).map(*).reduce(0, +)
    }
]

let result = try optimizer.minimize(objective, from: VectorN(Array(repeating: 1.0/Double(numAssets), count: numAssets)), constraints: constraints)
print("Optimal weights: \(result.solution)")
print("Portfolio variance: \(result.value)")
```

### Hyperparameter Tuning

Optimize machine learning hyperparameters:

```swift
// Tune learning rate, batch size, dropout rate for neural network
let searchSpace = [
    (0.0001, 0.1),      // Learning rate
    (16.0, 512.0),      // Batch size
    (0.0, 0.5)          // Dropout rate
]

let optimizer = GeneticAlgorithm<VectorN<Double>>(
    config: GeneticAlgorithmConfig(populationSize: 500, generations: 50),
    searchSpace: searchSpace
)

let objective = { (params: VectorN<Double>) -> Double in
    let learningRate = params[0]
    let batchSize = Int(params[1])
    let dropoutRate = params[2]

    // Train model and return validation loss
    return trainModel(lr: learningRate, batch: batchSize, dropout: dropoutRate)
}

let result = try optimizer.minimize(objective, from: VectorN([0.001, 128, 0.2]))
print("Best hyperparameters: LR=\(result.solution[0]), Batch=\(Int(result.solution[1])), Dropout=\(result.solution[2])")
```

### Function Minimization (Rosenbrock)

Classic optimization benchmark:

```swift
// Rosenbrock: f(x,y) = (1-x)² + 100(y-x²)²
// Global minimum at (1, 1)
let optimizer = GeneticAlgorithm<VectorN<Double>>(
    config: GeneticAlgorithmConfig(
        populationSize: 1000,
        generations: 500,
        mutationStrength: 0.2  // Higher for exploration
    ),
    searchSpace: [(-5.0, 5.0), (-5.0, 5.0)]
)

let rosenbrock = { (v: VectorN<Double>) -> Double in
    let x = v[0], y = v[1]
    return (1.0 - x) * (1.0 - x) + 100.0 * (y - x * x) * (y - x * x)
}

let result = try optimizer.minimize(rosenbrock, from: VectorN([0.0, 0.0]))
print("Solution: \(result.solution)")      // Near [1, 1]
print("Function value: \(result.value)")   // Near 0
```

---

## Performance Tuning

### Maximizing GPU Throughput

**1. Use Large Populations**

GPU parallelism scales with population size:

```swift
// Good: GPU efficiency ~90%
let config = GeneticAlgorithmConfig(populationSize: 2000, generations: 100)

// Bad: GPU overhead dominates
let config = GeneticAlgorithmConfig(populationSize: 200, generations: 500)
```

**2. Batch Generations**

Fewer generations with larger populations is faster than many generations with small populations:

```swift
// Better: 2000 × 100 = 200,000 evaluations in ~2 seconds
let config1 = GeneticAlgorithmConfig(populationSize: 2000, generations: 100)

// Worse: 500 × 400 = 200,000 evaluations in ~10 seconds
let config2 = GeneticAlgorithmConfig(populationSize: 500, generations: 400)
```

**3. Optimize Fitness Function**

Fitness evaluation is on CPU - keep it fast:

```swift
// Good: Simple arithmetic
let objective = { (v: VectorN<Double>) -> Double in
    v.dot(v) + v[0] * v[1]
}

// Bad: Complex simulation per evaluation
let objective = { (v: VectorN<Double>) -> Double in
    runExpensiveSimulation(params: v)  // 10ms each × 2000 = 20s per generation!
}
```

### Memory Management

GPU memory usage scales with population × dimension:

| Population | Dimension | GPU Memory |
|------------|-----------|------------|
| 1,000      | 10        | ~0.3 MB    |
| 2,000      | 50        | ~2.3 MB    |
| 10,000     | 100       | ~76 MB     |

Modern GPUs handle 10,000+ individuals easily.

---

## Monitoring and Debugging

### Convergence History

Track optimization progress:

```swift
let result = try optimizer.optimizeDetailed(objective: sphere)

print("Convergence history:")
for (gen, fitness) in result.convergenceHistory.enumerated() {
    print("Generation \(gen): Best fitness = \(fitness)")
}

// Plot convergence
// generations: [0, 1, 2, ..., 99]
// fitness: result.convergenceHistory
```

### Population Diversity

Monitor diversity to detect premature convergence:

```swift
print("Diversity history:")
for (gen, diversity) in result.diversityHistory.enumerated() {
    print("Generation \(gen): Diversity = \(diversity)")
}

// Low diversity → population converged
// High diversity → still exploring
```

### Deterministic Testing

Use seed for reproducible results:

```swift
let config = GeneticAlgorithmConfig(
    populationSize: 1000,
    generations: 100,
    seed: 12345  // Same seed → same result
)

let optimizer1 = GeneticAlgorithm<VectorN<Double>>(config: config, searchSpace: searchSpace)
let optimizer2 = GeneticAlgorithm<VectorN<Double>>(config: config, searchSpace: searchSpace)

let result1 = try optimizer1.minimize(objective, from: initialGuess)
let result2 = try optimizer2.minimize(objective, from: initialGuess)

// result1.solution == result2.solution (approximately)
```

---

## Best Practices

### ✅ Do

- **Use GPU for large populations** (≥ 1,000) - let automatic detection handle it
- **Tune parameters** - start with defaults, then experiment
- **Monitor convergence** - use `optimizeDetailed()` for tracking
- **Set search space carefully** - tighter bounds → faster convergence
- **Use constraints** - better than penalty in objective
- **Test deterministically** - use seed for debugging

### ❌ Don't

- **Don't force GPU for small populations** - CPU is faster
- **Don't over-constrain** - genetic algorithms explore broadly
- **Don't ignore diversity** - low diversity = premature convergence
- **Don't use tiny mutation rates** - prevents exploration
- **Don't expect exact constraint satisfaction** - penalty methods are approximate

---

## Troubleshooting

### GPU Not Activating?

**Check population size:**
```swift
// Too small - uses CPU
let config = GeneticAlgorithmConfig(populationSize: 500)  // < 1000

// Large enough - uses GPU
let config = GeneticAlgorithmConfig(populationSize: 1000)  // ≥ 1000
```

**Verify Metal availability:**
```swift
#if canImport(Metal)
if let device = MetalDevice.shared {
    print("Metal available: \(device.name)")
} else {
    print("Metal not available - using CPU fallback")
}
#else
print("Metal not supported on this platform")
#endif
```

### Slow Convergence?

1. **Increase population size** - more exploration
2. **Increase mutation rate** - escape local minima
3. **Decrease elite count** - reduce selection pressure
4. **Widen search space** - may be stuck in suboptimal region
5. **Try multiple runs** - stochastic algorithm may need restarts

### Poor Solutions?

1. **Run more generations** - may not have converged
2. **Check constraint formulation** - penalty may be too weak
3. **Verify search space** - ensure optimal solution is reachable
4. **Increase mutation strength** - explore more aggressively
5. **Use different seed** - may be stuck in poor region

---

## Performance Reference

### Benchmark Results (Apple M1 Pro)

| Problem | Population | Generations | Dimension | GPU Time | CPU Time | Speedup |
|---------|------------|-------------|-----------|----------|----------|---------|
| Sphere  | 1,000      | 100         | 2D        | 0.2s     | 0.5s     | 2.5×    |
| Sphere  | 2,000      | 100         | 5D        | 0.6s     | 3.2s     | 5.3×    |
| Sphere  | 2,000      | 100         | 10D       | 1.0s     | 15s      | 15×     |
| Rosenbrock | 1,000   | 500         | 2D        | 1.0s     | 12s      | 12×     |
| High-dim | 5,000     | 50          | 20D       | 2.5s     | 90s      | 36×     |

**Throughput**: 60,000-80,000 evaluations/second on GPU

---

## API Reference

### GeneticAlgorithm

```swift
public struct GeneticAlgorithm<V: VectorSpace>: MultivariateOptimizer
    where V.Scalar: Real
```

**Methods:**
- `minimize(_:from:constraints:)` - Minimize objective with optional constraints
- `optimizeDetailed(objective:)` - Get detailed results with convergence history

### GeneticAlgorithmConfig

```swift
public struct GeneticAlgorithmConfig: Sendable
```

**Properties:**
- `populationSize: Int` - Number of individuals (default: 100)
- `generations: Int` - Number of generations (default: 100)
- `crossoverRate: Double` - Crossover probability (default: 0.8)
- `mutationRate: Double` - Mutation probability per gene (default: 0.1)
- `mutationStrength: Double` - Gaussian σ (default: 0.1)
- `eliteCount: Int` - Top individuals preserved (default: 2)
- `tournamentSize: Int` - Tournament selection size (default: 3)
- `seed: UInt64?` - Random seed for reproducibility (default: nil)

**Presets:**
- `.default` - Balanced configuration
- `.highPerformance` - Large-scale optimization (pop=1000, gen=500)

### MultivariateOptimizationResult

```swift
public struct MultivariateOptimizationResult<V: VectorSpace>
```

**Properties:**
- `solution: V` - Best solution found
- `value: V.Scalar` - Objective function value at solution
- `iterations: Int` - Number of generations evolved
- `converged: Bool` - Whether algorithm converged
- `convergenceHistory: [V.Scalar]?` - Best fitness per generation (if `optimizeDetailed`)
- `diversityHistory: [V.Scalar]?` - Population diversity per generation (if `optimizeDetailed`)

---

## Further Reading

- **CHANGELOG.md** - Version history and release notes
- **EXAMPLES.md** - More code examples
- **TESTING.md** - Testing strategies and guidelines
- **Instruction Set/GPU_ACCELERATION_DETAILED.md** - Implementation details

---

## Support

For issues, questions, or feature requests:
- GitHub Issues: [BusinessMath](https://github.com/yourusername/BusinessMath/issues)
- Documentation: See DocC in Xcode
- Examples: Check `Examples/` directory

---

**Version:** 2.0.0
**Last Updated:** December 27, 2025
**GPU Support:** macOS 13+, iOS 14+, visionOS 1+ (Metal required)
