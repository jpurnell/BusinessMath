# Model Validation with Fake-Data Simulation

Check if your statistical models can recover parameters from simulated data.

## Overview

Before fitting a statistical model to real data, you should always verify that your fitting procedure works correctly. **Fake-data simulation** (also called simulation-based calibration or SBC) is a fundamental validation technique: simulate data from your model with known parameters, then check if your fitting procedure can recover those parameters.

This guide implements the workflow described by Andrew Gelman in his blog post "[Simulating from and checking a model in Stan](https://statmodeling.stat.columbia.edu/2025/12/15/simulating-from-and-checking-a-model-in-stan/)".

### What You'll Learn

- Why fake-data simulation is essential before using real data
- How to implement the parameter recovery workflow
- What problems this technique can catch
- How to interpret validation results

## The Fundamental Workflow

The fake-data simulation workflow has four steps:

1. **Specify true parameters** - Choose specific parameter values
2. **Simulate fake data** - Generate data from your model using those parameters
3. **Fit the model** - Apply your estimation procedure to the simulated data
4. **Check recovery** - Compare recovered parameters to true parameters

If your procedure **cannot** recover parameters from data you simulated yourself, something is wrong with your model, algorithm, or implementation.

```swift
// Step 1: Specify true parameters
let trueA = 0.2
let trueB = 0.3
let trueSigma = 0.2

// Steps 2-4: Run complete validation
let report = try ReciprocalParameterRecoveryCheck.run(
    trueA: trueA,
    trueB: trueB,
    trueSigma: trueSigma,
    n: 100,
    xRange: 0.0...10.0
)

print(report.summary)
// Shows whether each parameter was recovered within tolerance
```

## Why This Matters

### It's Not Obvious That Fitting Will Work

Even with correct model specification, fitting can fail for many reasons:

- **Poor identification**: Some parameter combinations produce identical data
- **Poor mixing**: Optimization gets stuck in local minima
- **Numerical instability**: Round-off errors accumulate
- **Coding bugs**: Implementation doesn't match mathematical specification
- **Algorithmic issues**: Optimizer can't handle the problem structure

### What This Check Tells You

**If you CAN'T recover parameters**:
- ❌ Your fitting procedure has fundamental problems
- Don't use it on real data until you fix the issues
- Investigate: Is it identification? Optimization? A bug?

**If you CAN recover parameters**:
- ✓ Your fitting procedure passes minimum sanity check
- ⚠️ This doesn't prove your model is appropriate for real data
- ⚠️ This doesn't guarantee you'll get good results on real data
- It just means your implementation works as intended

As Gelman notes: "It's a minimum check. If you're program *can't* fit data simulated from the model, then that's something that you'll want to know right away."

## The Reciprocal Regression Example

BusinessMath includes a complete implementation based on Gelman's Stan example: a nonlinear regression with reciprocal form.

### The Model

The model relates response `y` to predictor `x` through:

```
y[i] ~ Normal(μ[i], σ)
μ[i] = 1 / (a + b * x[i])
```

Where:
- `a` > 0: Intercept parameter
- `b` > 0: Slope parameter
- `σ` > 0: Residual standard deviation

This model appears in pharmacokinetics (Michaelis-Menten kinetics), economics (reciprocal demand curves), and other fields.

### Step-by-Step Implementation

#### 1. Simulate Data

Create a simulator with true parameter values:

```swift
let simulator = ReciprocalRegressionSimulator<Double>(
    a: 0.2,
    b: 0.3,
    sigma: 0.2
)

// Generate 100 observations with x uniform on [0, 10]
let data = simulator.simulate(n: 100, xRange: 0.0...10.0)

// Each data point contains (x, y)
for point in data.prefix(5) {
    print("x = \(point.x), y = \(point.y)")
}
```

#### 2. Fit the Model

Use maximum likelihood estimation to recover parameters:

```swift
let fitter = ReciprocalRegressionFitter<Double>()

let result = try fitter.fit(
    data: data,
    initialGuess: ReciprocalRegressionModel<Double>.Parameters(
        a: 0.5, b: 0.5, sigma: 0.5
    ),
    learningRate: 0.001,
    maxIterations: 1000
)

print("Fitted parameters:")
print("  a = \(result.parameters.a)")
print("  b = \(result.parameters.b)")
print("  sigma = \(result.parameters.sigma)")
print("  Converged: \(result.converged)")
```

#### 3. Validate Recovery

Compare recovered parameters to true values:

```swift
let trueParams = ["a": 0.2, "b": 0.3, "sigma": 0.2]
let recoveredParams = [
    "a": result.parameters.a,
    "b": result.parameters.b,
    "sigma": result.parameters.sigma
]

for (name, trueValue) in trueParams {
    let recovered = recoveredParams[name]!
    let relError = abs(recovered - trueValue) / abs(trueValue)
    let status = relError <= 0.10 ? "✓ PASS" : "✗ FAIL"

    print("\(name): true = \(trueValue), recovered = \(recovered) \(status)")
}
```

#### 4. Automated Validation

Or use the built-in validation framework:

```swift
let report = try ReciprocalParameterRecoveryCheck.run(
    trueA: 0.2,
    trueB: 0.3,
    trueSigma: 0.2,
    n: 100,
    xRange: 0.0...10.0,
    tolerance: 0.10  // 10% relative error
)

print(report.summary)
// Full report with convergence info, errors, pass/fail status
```

## Advanced Validation

### Multiple Replicates

Run the check multiple times to assess average performance:

```swift
let reports = try ReciprocalParameterRecoveryCheck.runMultiple(
    trueA: 0.2,
    trueB: 0.3,
    trueSigma: 0.2,
    replicates: 10,
    n: 100
)

print(ReciprocalParameterRecoveryCheck.summarizeReplicates(reports))
// Shows pass rate and average errors across replicates
```

This is closer to full simulation-based calibration (SBC), which Gelman describes as drawing parameters from the prior and checking coverage properties.

### Sample Size Effects

Investigate how much data you need for reliable recovery:

```swift
let sampleSizes = [20, 50, 100, 200, 500]

for n in sampleSizes {
    let report = try ReciprocalParameterRecoveryCheck.run(
        trueA: 0.2, trueB: 0.3, trueSigma: 0.2,
        n: n
    )

    print("N = \(n): \(report.passed ? "✓" : "✗")")
}
```

### Stress Testing

Try difficult scenarios to find where your procedure breaks:

```swift
// 1. High noise
try ReciprocalParameterRecoveryCheck.run(
    trueA: 0.2, trueB: 0.3, trueSigma: 1.0  // Large sigma
)

// 2. Extreme parameter values
try ReciprocalParameterRecoveryCheck.run(
    trueA: 0.001, trueB: 10.0, trueSigma: 0.1
)

// 3. Poor initialization
let fitter = ReciprocalRegressionFitter<Double>()
try fitter.fit(
    data: data,
    initialGuess: Parameters(a: 10.0, b: 10.0, sigma: 5.0),  // Way off
    maxIterations: 100
)
```

## Interpreting Results

### The Validation Report

The `ParameterRecoveryReport` contains:

- **True parameters**: Values used for simulation
- **Recovered parameters**: Values from fitting
- **Absolute errors**: |recovered - true|
- **Relative errors**: |recovered - true| / |true|
- **Within tolerance**: Boolean for each parameter
- **Convergence info**: Did optimization converge?
- **Overall status**: Did all parameters pass?

```swift
let report = try ReciprocalParameterRecoveryCheck.run(...)

if report.passed {
    print("✓ Model validation passed!")
    print("  All parameters recovered within \(report.tolerance * 100)% tolerance")
} else {
    print("✗ Model validation failed!")
    print("  Check which parameters failed:")
    for (name, passed) in report.withinTolerance {
        if !passed {
            print("  - \(name): \(report.relativeErrors[name]! * 100)% error")
        }
    }
}
```

### Common Failure Modes

**Non-convergence**:
```
Converged: No
Iterations: 1000 (hit maximum)
```
→ Try: More iterations, better initialization, different learning rate

**Large systematic bias**:
```
Parameter | True  | Recovered | Status
a         | 0.200 | 0.350     | ✗ FAIL
b         | 0.300 | 0.450     | ✗ FAIL
```
→ Possible identification problem or optimization getting stuck

**High variance**:
```
Replicate 1: a = 0.19 ✓
Replicate 2: a = 0.35 ✗
Replicate 3: a = 0.21 ✓
```
→ Unstable estimation, may need more data or better algorithm

## Best Practices

### 1. Always Validate Before Using Real Data

```swift
// DON'T:
let model = MyComplexModel()
let results = try model.fit(realWorldData)  // Hope it works!

// DO:
let validation = try MyComplexModel.validateOnFakeData()
if validation.passed {
    let results = try model.fit(realWorldData)
} else {
    print("Fix validation issues first!")
}
```

### 2. Use Realistic Parameter Values

Choose true parameters that reflect your actual application:

```swift
// If real data has small effects...
try ReciprocalParameterRecoveryCheck.run(
    trueA: 0.01, trueB: 0.02, trueSigma: 0.001  // Realistic scale
)
```

### 3. Test Edge Cases

Try extreme but plausible scenarios:

```swift
// Test boundaries
try validate(a: 0.001, b: 0.001, sigma: 0.001)  // Near zero
try validate(a: 10.0, b: 10.0, sigma: 1.0)      // Large values

// Test high noise
try validate(a: 0.2, b: 0.3, sigma: 2.0)        // SNR << 1
```

### 4. Document Your Validation

Include validation results in your analysis documentation:

```markdown
## Model Validation

Tested parameter recovery with:
- True a = 0.2, b = 0.3, sigma = 0.2
- N = 100 observations
- 10 replicates

Results:
- Pass rate: 100%
- Average relative error: a = 2.3%, b = 3.1%, sigma = 4.5%
- All converged in < 500 iterations
```

## Relationship to Other Validation Methods

### Fake-Data Simulation vs. Cross-Validation

- **Fake-data**: Tests if fitting procedure works at all
- **Cross-validation**: Tests out-of-sample predictive performance
- **Use both**: Fake-data first (sanity check), then cross-validation (model selection)

### Fake-Data Simulation vs. Full SBC

- **Fake-data (this guide)**: Single draw from specific parameters
- **Full SBC**: Many draws from prior, check coverage statistics
- **Fake-data is**: Quicker, easier, catches most problems
- **Full SBC is**: More rigorous, required for publication-quality validation

## Common Questions

**Q: Do I really need this? My model is simple.**

A: Yes! Even simple models can have implementation bugs, numerical issues, or identification problems you didn't anticipate. It takes minutes to check.

**Q: The validation passed. Does that mean my model is good?**

A: No. It means your **fitting procedure works**. The model could still be inappropriate for your real data. You still need domain knowledge, residual diagnostics, predictive checking, etc.

**Q: The validation failed. What should I do?**

A: Investigate systematically:
1. Check for coding bugs
2. Try different initializations
3. Increase iterations / adjust learning rate
4. Check if model is identified (can different parameters give same data?)
5. Try simpler model first
6. Examine optimization diagnostics

**Q: How close should recovered parameters be to true parameters?**

A: Depends on:
- Sample size (larger N → better recovery)
- Noise level (larger σ → worse recovery)
- Model complexity (more parameters → harder)
- Typical guideline: Within 10-20% relative error for moderate sample sizes

## See Also

- <doc:2.3-RiskAnalyticsGuide> - Risk measurement and validation
- <doc:4.1-MonteCarloTimeSeriesGuide> - Simulation techniques
- <doc:5.1-OptimizationGuide> - Optimization methods used for fitting
- <doc:5.5-Phase3-MultivariateOptimization> - Gradient descent details

## External References

- [Gelman's Blog Post](https://statmodeling.stat.columbia.edu/2025/12/15/simulating-from-and-checking-a-model-in-stan/) - Original inspiration
- [Simulation-Based Calibration](https://sites.stat.columbia.edu/gelman/research/published/2211.02383.pdf) - Full SBC methodology
- [Regression and Other Stories](https://avehtari.github.io/ROS-Examples/) - Chapter 7.2 on fake-data simulation

## Next Steps

1. Try the example: `Examples/FakeDataSimulationExample.swift`
2. Validate your own models by implementing similar checks
3. Read about full SBC for publication-quality validation
4. Combine with residual diagnostics and predictive checking
