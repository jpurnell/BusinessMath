# MultivariateOptimizer Protocol

Learn how to use the MultivariateOptimizer protocol for algorithm flexibility and polymorphism in optimization tasks.

## Overview

The ``MultivariateOptimizer`` protocol provides a unified interface for all multivariate optimization algorithms in BusinessMath. This enables algorithm swapping, comparison, and factory patterns while maintaining type safety.

All multivariate optimizers in BusinessMath conform to this protocol:
- **Unconstrained**: ``MultivariateGradientDescent``, ``MultivariateNewtonRaphson``
- **Constrained**: ``ConstrainedOptimizer``, ``InequalityOptimizer``
- **Advanced**: ``AdaptiveOptimizer``, ``ParallelOptimizer``, ``StochasticOptimizer``, ``RobustOptimizer``

## When to Use the Protocol

### Use the Protocol When:

- **Algorithm flexibility**: Choose algorithm at runtime based on problem characteristics
- **Testing**: Mock optimizers for unit testing business logic
- **Comparison**: Benchmark multiple algorithms on the same problem
- **Factory patterns**: Centralize algorithm selection logic

### Use Concrete Types When:

- **Algorithm-specific methods**: Access to `minimizeAdam()`, `minimizeBFGS()`, etc.
- **Specialized result types**: Lagrange multipliers from ``ConstrainedOptimizationResult``
- **Performance-critical code**: Avoid protocol dispatch overhead (minimal, but measurable)

## Basic Usage

Here's a complete, playground-ready example showing protocol usage:

```swift
import BusinessMath

// Sample problem: minimize f(x,y) = x² + y²
// Minimum is at (0, 0) with value 0

let objective = { (v: VectorN<Double>) -> Double in
    v.dot(v)  // Computes x² + y²
}

var initialGuess = VectorN([5.0, 5.0])

// Using the protocol for algorithm flexibility
let optimizer: any MultivariateOptimizer<VectorN<Double>> = MultivariateGradientDescent(
    learningRate: 0.01,
    maxIterations: 1000,
    tolerance: 0.0001
)

let result = try optimizer.minimize(objective, from: initialGuess)

print("Solution: \(result.solution.toArray().map({ $0.rounded() }))")           // Near [0, 0]
print("Objective value: \(result.objectiveValue.rounded())")  // Near 0
print("Iterations: \(result.iterations)")
print("Converged: \(result.converged)")
```

## Algorithm Swapping at Runtime

The protocol enables choosing algorithms dynamically:

```swift
import BusinessMath

// Function that selects optimizer based on problem size
func selectOptimizer(problemSize: Int) -> any MultivariateOptimizer<VectorN<Double>> {
    if problemSize < 10 {
        // Small problems: use Newton-Raphson for fast convergence
        return MultivariateNewtonRaphson(
            maxIterations: 100,
            tolerance: 1e-6
        )
    } else {
        // Large problems: use gradient descent for memory efficiency
        return MultivariateGradientDescent(
            learningRate: 0.01,
            maxIterations: 1000,
            tolerance: 1e-4
        )
    }
}

// Example: optimize a 2-variable function
let smallProblem = { (v: VectorN<Double>) -> Double in
	let x = v[0], y = v[1]
	return (x - 1) * (x - 1) + (y - 2) * (y - 2)
}

let optimizer = selectOptimizer(problemSize: 2)
let result = try optimizer.minimize(smallProblem, from: VectorN([0.0, 0.0]))

print("Solution: [\(result.solution[0].rounded()), \(result.solution[1].rounded())]")  // Near [1, 2]
```

## Factory Pattern Example

Use factories to encapsulate algorithm selection:

```swift
import BusinessMath

enum OptimizationStrategy {
    case fast      // Gradient descent
    case accurate  // Newton-Raphson
    case adaptive  // Automatic selection
}

struct OptimizerFactory {
    static func create(
        strategy: OptimizationStrategy,
        dimension: Int
    ) -> any MultivariateOptimizer<VectorN<Double>> {
        switch strategy {
        case .fast:
            return MultivariateGradientDescent<VectorN<Double>>(
                learningRate: 0.05,
                maxIterations: 1000
            )
        case .accurate:
            return MultivariateNewtonRaphson<VectorN<Double>>(
                maxIterations: 100,
                tolerance: 1e-8
            )
        case .adaptive:
            return AdaptiveOptimizer<VectorN<Double>>(
                maxIterations: 1000,
                tolerance: 1e-6
            )
        }
    }
}

// Use the factory
let objective = { (v: VectorN<Double>) -> Double in v.dot(v) }
let optimizer = OptimizerFactory.create(strategy: .adaptive, dimension: 2)
let result = try optimizer.minimize(objective, from: VectorN([3.0, 4.0]))

print("Objective value: \(result.objectiveValue)")
```

## Constrained Optimization

The protocol supports both unconstrained and constrained optimization:

```swift
import BusinessMath

// Minimize x² + y² subject to x + y = 1
let quadratic = { (v: VectorN<Double>) -> Double in v.dot(v) }

// Create constraint: x + y - 1 = 0
let budgetConstraint = MultivariateConstraint<VectorN<Double>>.equality { v in
	v[0] + v[1] - 1.0
}

// Use constrained optimizer via protocol
let constrainedOptimizer: any MultivariateOptimizer<VectorN<Double>> = InequalityOptimizer()

let constrainedResult = try constrainedOptimizer.minimize(
	quadratic,
	from: VectorN([0.5, 0.5]),
	constraints: [budgetConstraint]
)

print("Optimal weights: \(constrainedResult.solution.toArray().map({ $0.number(2) }))")  // Near [0.5, 0.5]
print("Sum of weights: \((constrainedResult.solution[0] + constrainedResult.solution[1]).rounded())")  // 1.0
```

## Comparing Multiple Algorithms

Benchmark different algorithms on the same problem:

```swift
import BusinessMath

// Rosenbrock function: f(x,y) = (1-x)² + 100(y-x²)²
// Global minimum at (1, 1) with value 0
let rosenbrock = { (v: VectorN<Double>) -> Double in
	let x = v[0], y = v[1]
	return (1 - x) * (1 - x) + 100 * (y - x*x) * (y - x*x)
}

let algorithms: [(name: String, optimizer: any MultivariateOptimizer<VectorN<Double>>)] = [
	("Gradient Descent", MultivariateGradientDescent(learningRate: 0.001, maxIterations: 10000)),
	("Newton-Raphson", MultivariateNewtonRaphson(maxIterations: 100, tolerance: 1e-6)),
	("Adaptive", AdaptiveOptimizer(maxIterations: 1000))
]

initialGuess = VectorN([0.0, 0.0])

print("Algorithm Comparison on Rosenbrock Function:")
print("-" + String(repeating: "-", count: 60))

for (name, optimizer) in algorithms {
	let result = try optimizer.minimize(rosenbrock, from: initialGuess)

	print("\(name):")
	print("  Solution: [\(result.solution[0].number(2)), \(result.solution[1].number(2))]")
	print("  Value: \(result.objectiveValue.rounded())")
	print("  Iterations: \(result.iterations)")
	print("  Converged: \(result.converged)")
	print()
}
```

## Portfolio Optimization Example

Real-world usage in portfolio allocation:

```swift
import BusinessMath

// Asset returns and covariance matrix
let expectedReturns = VectorN([0.08, 0.12, 0.15])
let covariance = [
	[0.04, 0.01, 0.02],
	[0.01, 0.09, 0.03],
	[0.02, 0.03, 0.16]
]

// Portfolio variance function
let portfolioVariance = { (weights: VectorN<Double>) -> Double in
	let w = weights.toArray()
	var variance = 0.0
	for i in 0..<3 {
		for j in 0..<3 {
			variance += w[i] * covariance[i][j] * w[j]
		}
	}
	return variance
}

// Constraints: weights sum to 1, all non-negative
let constraints = [
	MultivariateConstraint<VectorN<Double>>.budgetConstraint
] + MultivariateConstraint<VectorN<Double>>.nonNegativity(dimension: 3)

// Use protocol for algorithm flexibility
let optimizer: any MultivariateOptimizer<VectorN<Double>> = InequalityOptimizer(
	maxIterations: 100
)

let result = try optimizer.minimize(
	portfolioVariance,
	from: VectorN([1.0/3, 1.0/3, 1.0/3]),
	constraints: constraints
)

print("Optimal portfolio weights:")
for (i, weight) in result.solution.toArray().enumerated() {
	print("  Asset \(i+1): \(weight.percent())")
}

let portfolioReturn = expectedReturns.dot(result.solution)
let portfolioStdDev = sqrt(result.objectiveValue)

print("\nPortfolio metrics:")
print("  Expected return: \(portfolioReturn.percent())")
print("  Volatility: \(portfolioStdDev.percent())")
print("  Sharpe ratio: \((portfolioReturn / portfolioStdDev).number(3))")
```

## Algorithm Selection Guide

Choose the right optimizer for your problem:

| Algorithm            | Best For                        | Constraints   | Speed   | Accuracy |
|----------------------|---------------------------------|---------------|---------|----------|
| **Gradient Descent** | Large problems (100+ variables) | Unconstrained | Fast    | Medium   |
| **Newton-Raphson**   | Small problems (<10 variables)  | Unconstrained | Medium  | High     |
| **Constrained**      | Equality constraints only       | Equality      | Medium  | High     |
| **Inequality**       | Mixed constraints               | Both          | Slow    | High     |
| **Adaptive**         | Unknown problem characteristics | Both          | Medium  | High     |
| **Parallel**         | Global optimization             | Both          | Fast+   | High     |
| **Stochastic**       | Uncertain parameters            | Both          | Slow    | Medium   |
| **Robust**           | Worst-case optimization         | Both          | Slow    | Medium   |

† Requires async/await and multiple CPU cores

## Complete Playground Example

Here's a complete example you can paste directly into an Xcode Playground:

```swift
import BusinessMath

// EXAMPLE 1: Basic protocol usage
print("=== Example 1: Basic Protocol Usage ===\n")

let quadratic = { (v: VectorN<Double>) -> Double in v.dot(v) }
let optimizer1: any MultivariateOptimizer<VectorN<Double>> = MultivariateGradientDescent(
	learningRate: 0.01,
	maxIterations: 1000
)
let result1 = try optimizer1.minimize(quadratic, from: VectorN([5.0, 5.0]))
print("Solution: \(result1.solution.toArray().map({ $0.number(2)}))")
print("Value: \(result1.objectiveValue.rounded())\n")

// EXAMPLE 2: Algorithm comparison
print("=== Example 2: Algorithm Comparison ===\n")

var algorithms: [(name: String, optimizer: any MultivariateOptimizer<VectorN<Double>>)] = [
	("Gradient Descent", MultivariateGradientDescent(learningRate: 0.001, maxIterations: 10000)),
	("Newton-Raphson", MultivariateNewtonRaphson(maxIterations: 100, tolerance: 1e-6)),
	("Adaptive", AdaptiveOptimizer(maxIterations: 1000))
]
for (name, optimizer) in algorithms {
		let result = try optimizer.minimize(quadratic, from: VectorN([3.0, 4.0]))
		print("\(type(of: optimizer)): \(result.iterations) iterations")
	}

print()

// EXAMPLE 3: Constrained optimization
print("=== Example 3: Constrained Optimization ===\n")

let constraint = MultivariateConstraint<VectorN<Double>>.equality { v in
	v[0] + v[1] - 1.0  // x + y = 1
}

let optimizer3: any MultivariateOptimizer<VectorN<Double>> = InequalityOptimizer()
let result3 = try optimizer3.minimize(
	quadratic,
	from: VectorN([0.5, 0.5]),
	constraints: [constraint]
)

print("Constrained solution: \(result3.solution.toArray().map({ $0.number(2)}))")
print("Constraint check (x+y): \((result3.solution[0] + result3.solution[1]).rounded())")
```

## Next Steps

- Follow <doc:5.5-MultivariateOptimization> for complete optimization tutorial
- Learn <doc:5.6-ConstrainedOptimization> for handling complex constraints
- Explore <doc:5.2-PortfolioOptimizationGuide> for real-world portfolio applications
- Read <doc:5.4-VectorOperations> to understand vector spaces in optimization

## See Also

- ``MultivariateOptimizer``
- ``MultivariateOptimizationResult``
- ``MultivariateGradientDescent``
- ``MultivariateNewtonRaphson``
- ``ConstrainedOptimizer``
- ``InequalityOptimizer``
- ``AdaptiveOptimizer``
- ``ParallelOptimizer``
- ``StochasticOptimizer``
- ``RobustOptimizer``
- ``MultivariateConstraint``
