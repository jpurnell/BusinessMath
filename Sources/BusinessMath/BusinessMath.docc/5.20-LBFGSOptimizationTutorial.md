# L-BFGS Optimization

Learn how to use the L-BFGS quasi-Newton optimizer for efficient gradient-based optimization.

## Overview

L-BFGS (Limited-memory Broyden–Fletcher–Goldfarb–Shanno) is a quasi-Newton optimization algorithm that approximates the inverse Hessian using a limited history of gradient evaluations. It's particularly effective for smooth, differentiable functions and scales well to higher dimensions.

**When to use L-BFGS:**
- Your function is smooth and differentiable
- You need faster convergence than gradient descent
- Memory is limited (compared to full BFGS)
- Problem dimension is moderate (1-1000 variables)

**When to avoid L-BFGS:**
- Function is non-differentiable or has discontinuities
- Gradients are noisy or unreliable
- Problem is highly non-convex with many local minima

## Algorithm Overview

L-BFGS maintains a limited history of past gradients to approximate the inverse Hessian matrix without computing it explicitly. This enables Newton-like convergence with minimal memory requirements.

### Key Features

- **Memory efficient**: Stores only m previous iterations (typically 5-20)
- **Fast convergence**: Superlinear convergence near the minimum
- **Two-loop recursion**: Efficiently computes search direction
- **Line search**: Backtracking to ensure sufficient decrease

## Basic Usage

> **Note for Xcode Playgrounds:** L-BFGS uses async/await for optimization. All playground examples require:
> ```swift
> import PlaygroundSupport
> PlaygroundPage.current.needsIndefiniteExecution = true
> Task {
>     // ... your async L-BFGS code ...
>     PlaygroundPage.current.finishExecution()
> }
> ```
> Examples throughout this tutorial include these wrappers automatically.

### Simple Quadratic Minimization

The simplest use case is minimizing a quadratic function:

```swift
import BusinessMath
import PlaygroundSupport

PlaygroundPage.current.needsIndefiniteExecution = true

Task {
    // Define objective function: f(x) = (x - 4)²
    let objective: @Sendable (Double) -> Double = { x in
        (x - 4.0) * (x - 4.0)
    }

    // Create L-BFGS optimizer
    let optimizer = AsyncLBFGSOptimizer(
        memorySize: 10,
        tolerance: 1e-6,
        maxIterations: 100
    )

    // Optimize
    let result = try await optimizer.optimize(
        objective: objective,
        constraints: [],
        initialGuess: 0.0,
        bounds: nil
    )

    print("Optimal value: \(result.optimalValue)")  // ≈ 4.0
    print("Objective at optimum: \(result.objectiveValue)")  // ≈ 0.0
    print("Converged: \(result.converged)")
    print("Iterations: \(result.iterations)")

    PlaygroundPage.current.finishExecution()
}
```

### Configuring Memory Size

The memory size parameter controls how many previous iterations are stored:

```swift
// Small memory (faster, less accurate)
let fastOptimizer = AsyncLBFGSOptimizer(
    memorySize: 5,
    tolerance: 1e-4,
    maxIterations: 100
)

// Larger memory (slower, more accurate)
let accurateOptimizer = AsyncLBFGSOptimizer(
    memorySize: 20,
    tolerance: 1e-8,
    maxIterations: 200
)
```

**Memory size guidelines:**
- **m = 3-5**: Fast but less accurate, good for large problems
- **m = 10**: Default, balances speed and accuracy
- **m = 20-50**: High accuracy, suitable for smaller problems

## Adaptive Progress Reporting

L-BFGS supports real-time progress monitoring with configurable reporting strategies.

### Fixed Interval Progress

Report progress every N iterations:

```swift
import BusinessMath
import PlaygroundSupport

PlaygroundPage.current.needsIndefiniteExecution = true

Task {
    let objective: @Sendable (Double) -> Double = { x in
        (x - 4.0) * (x - 4.0)
    }

    let optimizer = AsyncLBFGSOptimizer(
        memorySize: 10,
        tolerance: 1e-6,
        maxIterations: 100,
        progressStrategy: FixedIntervalStrategy(interval: 10)
    )

    let result = try await optimizer.optimizeWithProgress(
        objective: objective,
        constraints: [],
        initialGuess: 0.0,
        bounds: nil
    ) { progress in
        print("Iteration \(progress.iteration): " +
              "position = \(progress.position), " +
              "gradient = \(progress.gradient)")
    }

    PlaygroundPage.current.finishExecution()
}
```

### Exponential Backoff Progress

Report frequently at first, then less often as optimization progresses:

```swift
let optimizer = AsyncLBFGSOptimizer(
    memorySize: 10,
    tolerance: 1e-6,
    maxIterations: 1000,
    progressStrategy: ExponentialBackoffStrategy(
        initialInterval: 1,    // Report first iteration
        maxInterval: 100,      // Maximum gap between reports
        backoffFactor: 2.0     // Double interval each time
    )
)
```

This strategy reports at iterations: 1, 2, 4, 8, 16, 32, 64, 100, 200, 300...

### Convergence-Based Progress

Report more frequently when convergence is slow:

```swift
let optimizer = AsyncLBFGSOptimizer(
    memorySize: 10,
    tolerance: 1e-6,
    maxIterations: 1000,
    progressStrategy: ConvergenceBasedStrategy(
        minInterval: 5,
        maxInterval: 50,
        convergenceThreshold: 0.01
    )
)
```

## Early Stopping with Convergence Detection

Stop optimization early when progress stagnates:

```swift
import BusinessMath
import PlaygroundSupport

PlaygroundPage.current.needsIndefiniteExecution = true

Task {
    let objective: @Sendable (Double) -> Double = { x in
        (x - 4.0) * (x - 4.0)
    }

    let detector = ConvergenceDetector(
        windowSize: 10,
        improvementThreshold: 0.001,
        gradientThreshold: 0.01
    )

    let optimizer = AsyncLBFGSOptimizer(
        memorySize: 10,
        tolerance: 1e-10,  // Very tight
        maxIterations: 1000,
        convergenceDetector: detector
    )

    let result = try await optimizer.optimize(
        objective: objective,
        constraints: [],
        initialGuess: 0.0,
        bounds: nil
    )

    // Will stop early if improvement < 0.001 for 10 iterations
    // or gradient norm < 0.01
    print("Converged early: \(result.converged)")
    print("Iterations: \(result.iterations)")

    PlaygroundPage.current.finishExecution()
}
```

## AsyncSequence Streaming

Monitor optimization progress in real-time using AsyncSequence:

```swift
import BusinessMath
import PlaygroundSupport

PlaygroundPage.current.needsIndefiniteExecution = true

Task {
    let objective: @Sendable (Double) -> Double = { x in
        (x - 4.0) * (x - 4.0)
    }

    let optimizer = AsyncLBFGSOptimizer(
        memorySize: 10,
        tolerance: 1e-6,
        maxIterations: 100,
        progressStrategy: FixedIntervalStrategy(interval: 5)
    )

    let stream = optimizer.optimizeWithProgressStream(
        objective: objective,
        constraints: [],
        initialGuess: 10.0,
        bounds: nil
    )

    for try await progress in stream {
        if let result = progress.result {
            // Final result available
            print("Optimization complete!")
            print("Solution: \(result.optimalValue)")
        } else {
            // Intermediate progress
            print("Iteration \(progress.iteration): " +
                  "gradient norm = \(progress.metrics.gradientNorm)")
        }
    }

    PlaygroundPage.current.finishExecution()
}
```

## Advanced Examples

### Rosenbrock Function

The Rosenbrock function is a classic challenging test case:

```swift
import BusinessMath
import PlaygroundSupport

PlaygroundPage.current.needsIndefiniteExecution = true

Task {
    // f(x) = (1-x)² + 100(2-x²)²  (1D version)
    // Global minimum at x = 1
    let rosenbrock: @Sendable (Double) -> Double = { x in
        let term1 = (1.0 - x) * (1.0 - x)
        let term2 = 100.0 * (2.0 - x * x) * (2.0 - x * x)
        return term1 + term2
    }

    let optimizer = AsyncLBFGSOptimizer(
        memorySize: 10,
        tolerance: 1e-4,
        maxIterations: 200
    )

    let result = try await optimizer.optimize(
        objective: rosenbrock,
        constraints: [],
        initialGuess: 0.0,
        bounds: nil
    )

    print("Found minimum at: \(result.optimalValue)")  // ≈ 1.0

    PlaygroundPage.current.finishExecution()
}
```

### With Tight Tolerance

For high-precision results:

```swift
import BusinessMath
import PlaygroundSupport

PlaygroundPage.current.needsIndefiniteExecution = true

Task {
    let optimizer = AsyncLBFGSOptimizer(
        memorySize: 20,
        tolerance: 1e-10,  // Very tight
        maxIterations: 500
    )

    let result = try await optimizer.optimize(
        objective: { x in (x - 7.0) * (x - 7.0) },
        constraints: [],
        initialGuess: 0.0,
        bounds: nil
    )

    print("Optimal value: \(result.optimalValue)")  // ≈ 7.0
    print("Objective: \(result.objectiveValue)")   // ≈ 0.0

    PlaygroundPage.current.finishExecution()
}

print("High-precision result: \(result.optimalValue)")  // Very close to 7.0
print("Function value: \(result.objectiveValue)")  // ≈ 0.0 within 1e-10
```

## Performance Characteristics

L-BFGS performance depends on several factors:

### Convergence Rate

- **Quadratic functions**: Converges in few iterations (often < 10)
- **Smooth non-quadratic**: Superlinear convergence near minimum
- **Ill-conditioned problems**: May require more iterations

### Memory vs Accuracy Trade-off

| Memory Size | Speed | Accuracy | Best For |
|-------------|-------|----------|----------|
| 3-5 | Fast | Good | Large-scale problems |
| 10-15 | Medium | Better | General use |
| 20-50 | Slower | Best | Small, precise problems |

### Iteration Count Examples

```swift
// Simple quadratic: ~5-10 iterations
(x - 4)²

// Rosenbrock: ~50-100 iterations
(1-x)² + 100(2-x²)²

// Already optimal: 1-2 iterations
Starting at x = 4 for (x - 4)²
```

## Troubleshooting

### Slow Convergence

If L-BFGS converges slowly:

1. **Increase memory size**: Try m = 20 or higher
2. **Adjust tolerance**: Relax if high precision isn't needed
3. **Check initial guess**: Start closer to the optimum
4. **Consider rescaling**: Normalize your variables

### Oscillation or Divergence

If optimization oscillates:

1. **Reduce memory size**: Try m = 5
2. **Check function smoothness**: L-BFGS requires differentiable functions
3. **Verify gradients**: Numerical gradient should be accurate

### Early Termination

If optimization stops too early:

1. **Tighten tolerance**: Use 1e-8 or smaller
2. **Increase max iterations**: Allow more time
3. **Disable convergence detector**: Remove if too aggressive

## Comparison with Other Optimizers

### vs. Gradient Descent
- **L-BFGS**: Faster convergence, more memory
- **GD**: Simpler, less memory, slower

### vs. Conjugate Gradient
- **L-BFGS**: Better for general problems
- **CG**: Better for very large, sparse problems

### vs. Newton's Method
- **L-BFGS**: Doesn't need Hessian computation
- **Newton**: Faster but requires expensive Hessian

## Next Steps

- Explore <doc:5.21-ConjugateGradientTutorial> for an alternative gradient-based method
- Learn about <doc:5.22-SimulatedAnnealingTutorial> for global optimization
- Try <doc:5.23-NelderMeadTutorial> for derivative-free optimization
- Review <doc:5.5-MultivariateOptimization> for multivariate problems

## See Also

- ``AsyncLBFGSOptimizer``
- ``LBFGSProgress``
- ``ProgressStrategy``
- ``FixedIntervalStrategy``
- ``ExponentialBackoffStrategy``
- ``ConvergenceBasedStrategy``
- ``ConvergenceDetector``
- ``ConvergenceMetrics``
- ``OptimizationResult``
