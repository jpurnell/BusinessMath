# Nelder-Mead Optimization

Learn how to use the Nelder-Mead simplex optimizer for derivative-free optimization of continuous functions.

## Overview

Nelder-Mead is a deterministic direct search method that maintains a simplex (n+1 vertices in n dimensions) and iteratively transforms it to find the minimum. Unlike gradient-based methods, it only requires function evaluations, making it ideal for non-smooth or noisy objectives.

**When to use Nelder-Mead:**
- Function is non-differentiable or discontinuous
- Gradients are unavailable or expensive to compute
- Objective evaluations are noisy
- Problem dimension is small to moderate (1-20 variables)
- You need a simple, robust method without tuning

**When to avoid Nelder-Mead:**
- Function is smooth with available gradients (use L-BFGS or CG)
- High dimensions (>20 variables) - convergence slows significantly
- Global optimization needed (use Simulated Annealing instead)
- Very high precision required (NM can stagnate)

## Algorithm Overview

Nelder-Mead maintains a simplex of n+1 points in n-dimensional space. At each iteration, it evaluates four possible transformations of the worst vertex and chooses the best one. This geometric approach requires no derivatives.

### Key Features

- **Derivative-free**: Only requires function evaluations
- **Deterministic**: Same starting point gives same result
- **Four simplex operations**: Reflection, expansion, contraction, shrink
- **Adaptive geometry**: Simplex adjusts shape automatically
- **Constraint support**: Handles equality/inequality constraints via penalty method
- **Configurable**: Three preset configurations (.default, .highPrecision, .fast)

### Simplex Operations

1. **Reflection** (α=1.0): Reflect worst point through centroid
2. **Expansion** (γ=2.0): Extend reflection if promising
3. **Contraction** (ρ=0.5): Pull toward better points
4. **Shrink** (σ=0.5): Contract entire simplex toward best point

## Basic Usage

### Simple Quadratic Minimization

Minimize a simple function in 2D:

```swift
import BusinessMath

// Define objective: f(x,y) = x² + y²
let sphere: (VectorN<Double>) -> Double = { v in
    v.dot(v)
}

// Create Nelder-Mead optimizer
let optimizer = NelderMead<VectorN<Double>>(config: .default)

// Optimize
let result = try optimizer.minimize(
    sphere,
    from: VectorN([5.0, 5.0])
)

print("Optimal value: \(result.solution)")  // ≈ [0.0, 0.0]
print("Objective at optimum: \(result.value)")  // ≈ 0.0
print("Converged: \(result.converged)")
print("Iterations: \(result.iterations)")
```

### Configuration Presets

Choose from three preset configurations:

```swift
// Default configuration (good starting point)
let defaultOptimizer = NelderMead<VectorN<Double>>(
    config: .default  // tolerance=1e-6, maxIterations=500
)

// High precision configuration
let preciseOptimizer = NelderMead<VectorN<Double>>(
    config: .highPrecision  // tolerance=1e-10, maxIterations=2000
)

// Fast configuration
let fastOptimizer = NelderMead<VectorN<Double>>(
    config: .fast  // tolerance=1e-4, maxIterations=200
)
```

## Simplex Coefficients

Customize the four simplex transformation coefficients:

```swift
// Custom coefficients for aggressive exploration
let aggressiveOptimizer = NelderMead<VectorN<Double>>(
    config: NelderMeadConfig(
        reflectionCoefficient: 1.5,    // More aggressive reflection
        expansionCoefficient: 2.5,      // Larger expansion
        contractionCoefficient: 0.4,    // Tighter contraction
        shrinkCoefficient: 0.6,         // Less aggressive shrink
        tolerance: 1e-6,
        maxIterations: 500
    )
)

// Conservative coefficients for stability
let conservativeOptimizer = NelderMead<VectorN<Double>>(
    config: NelderMeadConfig(
        reflectionCoefficient: 1.0,     // Standard reflection
        expansionCoefficient: 1.5,      // Conservative expansion
        contractionCoefficient: 0.5,    // Standard contraction
        shrinkCoefficient: 0.5,         // Standard shrink
        tolerance: 1e-6,
        maxIterations: 1000
    )
)
```

**Coefficient guidelines:**
- **Reflection (α)**: Typically 1.0-1.5
- **Expansion (γ)**: Typically 1.5-2.5
- **Contraction (ρ)**: Typically 0.4-0.6
- **Shrink (σ)**: Typically 0.4-0.6

Standard values (α=1, γ=2, ρ=0.5, σ=0.5) work well for most problems.

## Initial Simplex Size

Control the initial simplex size:

```swift
// Small initial simplex (for local search)
let localOptimizer = NelderMead<VectorN<Double>>(
    config: NelderMeadConfig(
        initialSimplexSize: 0.5,  // Small initial spread
        tolerance: 1e-8,
        maxIterations: 500
    )
)

// Large initial simplex (for global search)
let globalOptimizer = NelderMead<VectorN<Double>>(
    config: NelderMeadConfig(
        initialSimplexSize: 2.0,  // Large initial spread
        tolerance: 1e-6,
        maxIterations: 1000
    )
)
```

**Simplex size guidelines:**
- **0.1-0.5**: Fine-grained local search
- **1.0**: Default, balanced exploration (default)
- **1.5-2.0**: Broad global exploration
- **>2.0**: Very aggressive, may be inefficient

## Advanced Examples

### Rosenbrock Function

The Rosenbrock function is a classic challenging test case:

```swift
// Rosenbrock: f(x,y) = (1-x)² + 100(y-x²)²
// Narrow curved valley, global minimum at (1, 1)
let rosenbrock: (VectorN<Double>) -> Double = { v in
    let x = v[0], y = v[1]
    return (1.0 - x) * (1.0 - x) + 100.0 * (y - x * x) * (y - x * x)
}

let optimizer = NelderMead<VectorN<Double>>(
    config: NelderMeadConfig(
        tolerance: 1e-6,
        maxIterations: 1000  // Rosenbrock needs more iterations
    )
)

let result = try optimizer.minimize(
    rosenbrock,
    from: VectorN([0.0, 0.0])
)

print("Found minimum at: \(result.solution)")  // ≈ [1.0, 1.0]
print("Objective value: \(result.value)")  // ≈ 0.0
```

### Non-Smooth Function

Nelder-Mead handles non-smooth objectives:

```swift
// Absolute value function: f(x,y) = |x| + |y|
// Non-differentiable at (0, 0)
let absoluteSum: (VectorN<Double>) -> Double = { v in
    abs(v[0]) + abs(v[1])
}

let optimizer = NelderMead<VectorN<Double>>(config: .default)

let result = try optimizer.minimize(
    absoluteSum,
    from: VectorN([3.0, 3.0])
)

// Successfully finds non-differentiable minimum
print("Solution: \(result.solution)")  // ≈ [0.0, 0.0]
```

### High-Precision Optimization

For very tight tolerances:

```swift
let optimizer = NelderMead<VectorN<Double>>(
    config: NelderMeadConfig(
        initialSimplexSize: 0.5,  // Small simplex for precision
        tolerance: 1e-10,          // Very tight tolerance
        maxIterations: 2000
    )
)

let quadratic: (VectorN<Double>) -> Double = { v in
    (v[0] - 7.0) * (v[0] - 7.0) + (v[1] - 3.0) * (v[1] - 3.0)
}

let result = try optimizer.minimize(
    quadratic,
    from: VectorN([0.0, 0.0])
)

print("High-precision result: \(result.solution)")  // Very close to [7.0, 3.0]
```

## Detailed Results

Access detailed optimization information:

```swift
let optimizer = NelderMead<VectorN<Double>>(config: .default)

let result = optimizer.optimizeDetailed(
    objective: rosenbrock,
    initialGuess: VectorN([0.0, 0.0])
)

print("Solution: \(result.solution)")
print("Value: \(result.value)")
print("Iterations: \(result.iterations)")
print("Evaluations: \(result.evaluations)")
print("Converged: \(result.converged)")
print("Convergence reason: \(result.convergenceReason)")
print("Final simplex size: \(result.finalSimplexSize)")

// Access convergence history
if let history = result.convergenceHistory {
    print("Best value over time: \(history)")
}
```

## Constrained Optimization

Nelder-Mead handles constraints via penalty method:

```swift
// Minimize x² + y² subject to x + y = 2
let objective: (VectorN<Double>) -> Double = { v in
    v.dot(v)
}

let constraint: MultivariateConstraint<VectorN<Double>> = .equality(
    function: { v in v[0] + v[1] - 2.0 },
    gradient: { _ in VectorN([1.0, 1.0]) }
)

let optimizer = NelderMead<VectorN<Double>>(config: .default)

let result = try optimizer.minimize(
    objective,
    from: VectorN([3.0, 3.0]),
    constraints: [constraint]
)

// Solution approximately (1, 1) satisfying x + y = 2
print("Solution: \(result.solution)")
let constraintValue = result.solution[0] + result.solution[1]
print("Constraint value: \(constraintValue)")  // ≈ 2.0
```

### Inequality Constraints

Handle inequality constraints:

```swift
// Minimize (x-3)² + (y-3)² subject to x + y ≤ 4
let objective: (VectorN<Double>) -> Double = { v in
    (v[0] - 3.0) * (v[0] - 3.0) + (v[1] - 3.0) * (v[1] - 3.0)
}

let constraint: MultivariateConstraint<VectorN<Double>> = .inequality(
    function: { v in v[0] + v[1] - 4.0 },  // g(x) ≤ 0
    gradient: { _ in VectorN([1.0, 1.0]) }
)

let optimizer = NelderMead<VectorN<Double>>(config: .default)

let result = try optimizer.minimize(
    objective,
    from: VectorN([0.0, 0.0]),
    constraints: [constraint]
)

// Constrained minimum on boundary near (2, 2)
print("Solution: \(result.solution)")
```

## Dimensional Considerations

Nelder-Mead's effectiveness decreases with dimension:

### 1D Optimization

```swift
// Simple 1D case
let objective: (VectorN<Double>) -> Double = { v in
    (v[0] - 7.0) * (v[0] - 7.0)
}

let optimizer = NelderMead<VectorN<Double>>(config: .default)
let result = try optimizer.minimize(objective, from: VectorN([0.0]))

// Converges quickly in 1D
print("Solution: \(result.solution[0])")  // ≈ 7.0
```

### Higher Dimensions

```swift
// 5D sphere function
let objective: (VectorN<Double>) -> Double = { v in
    v.dot(v)
}

let optimizer = NelderMead<VectorN<Double>>(
    config: NelderMeadConfig(
        tolerance: 1e-4,      // Relax tolerance for higher dimensions
        maxIterations: 1000   // More iterations needed
    )
)

let initialGuess = VectorN(Array(repeating: 2.0, count: 5))
let result = try optimizer.minimize(objective, from: initialGuess)
```

**Dimensional scaling guidelines:**
- **1-3D**: Excellent performance, fast convergence
- **4-10D**: Good performance with default settings
- **10-20D**: Slower, may need more iterations
- **>20D**: Consider gradient-based or stochastic methods

## Convergence Criteria

Nelder-Mead uses two convergence criteria:

### Simplex Size Convergence

Stops when simplex becomes very small:

```swift
let optimizer = NelderMead<VectorN<Double>>(
    config: NelderMeadConfig(
        tolerance: 1e-8  // Simplex diameter must be < 1e-8
    )
)

// Converges when max distance between vertices < tolerance
```

### Function Value Convergence

Stops when function values at all vertices are similar:

```swift
// Converges when range of function values across simplex is small
let result = optimizer.optimizeDetailed(
    objective: objective,
    initialGuess: initialGuess
)

print("Convergence reason: \(result.convergenceReason)")
// May show "Simplex size below tolerance" or similar
```

## Performance Characteristics

Nelder-Mead performance depends on several factors:

### Convergence Rate

- **Quadratic functions**: ~10-50 iterations
- **Smooth non-quadratic**: ~50-200 iterations
- **Non-smooth functions**: ~100-500 iterations
- **Rosenbrock**: ~200-1000 iterations

### Iteration Scaling by Dimension

| Dimension | Typical Iterations | Speed |
|-----------|-------------------|-------|
| 1-2D | 20-100 | Fast |
| 3-5D | 50-200 | Medium |
| 6-10D | 100-500 | Slower |
| 11-20D | 200-1000 | Slow |
| >20D | >1000 | Very slow |

### Function Evaluation Count

Nelder-Mead requires multiple evaluations per iteration:

```swift
let result = optimizer.optimizeDetailed(objective: f, initialGuess: x0)
print("Iterations: \(result.iterations)")
print("Evaluations: \(result.evaluations)")
// Evaluations ≈ iterations × (dimension + 1)
```

## Troubleshooting

### Slow Convergence

If Nelder-Mead converges slowly:

1. **Relax tolerance**: Use 1e-4 instead of 1e-6
2. **Increase max iterations**: Allow more time
3. **Adjust simplex size**: Try initialSimplexSize = 0.5 or 2.0
4. **Check dimensionality**: NM struggles above 20D
5. **Consider gradient methods**: If smooth, use L-BFGS

### Premature Convergence

If optimization stops too early:

1. **Tighten tolerance**: Use 1e-8 or 1e-10
2. **Increase max iterations**: Allow more exploration
3. **Larger initial simplex**: Try initialSimplexSize = 2.0
4. **Adjust coefficients**: Increase expansion coefficient

### Oscillation or Stagnation

If simplex oscillates without converging:

1. **Reduce expansion coefficient**: Try γ = 1.5 instead of 2.0
2. **Increase shrink frequency**: Adjust shrinkCoefficient
3. **Check for flat regions**: May need different method
4. **Verify objective function**: Check for numerical issues

### Poor Solutions

If Nelder-Mead finds suboptimal points:

1. **Try different starting points**: NM is local optimizer
2. **Use Simulated Annealing**: For global optimization
3. **Increase initial simplex size**: Explore more broadly
4. **Check constraints**: Penalty method may need tuning

## Comparison with Other Optimizers

### vs. L-BFGS / Conjugate Gradient
- **NM**: Derivative-free, works on non-smooth functions, slower
- **L-BFGS/CG**: Requires gradients, much faster on smooth functions

### vs. Simulated Annealing
- **NM**: Deterministic, local optimization, faster convergence
- **SA**: Stochastic, global optimization, escapes local minima

### vs. Genetic Algorithms
- **NM**: Simplex geometry, deterministic, low dimensions
- **GA**: Population-based, stochastic, better for discrete or high-D

### vs. Gradient Descent
- **NM**: No gradients needed, handles non-smooth
- **GD**: Requires gradients, faster on smooth objectives

## When to Choose Nelder-Mead

Choose Nelder-Mead when:
- Gradients unavailable or expensive
- Function is non-smooth or discontinuous
- Objective evaluations are noisy
- Problem dimension is small (<20 variables)
- You need a simple, robust method

Avoid Nelder-Mead when:
- Function is smooth with cheap gradients (use L-BFGS)
- Very high dimensions (>20 variables)
- Global optimization needed (use Simulated Annealing)
- Extremely tight precision required (NM can stagnate)

## Practical Tips

### Starting Points

```swift
// Try multiple starting points for better results
let startingPoints = [
    VectorN([0.0, 0.0]),
    VectorN([1.0, 1.0]),
    VectorN([-1.0, -1.0]),
    VectorN([2.0, -2.0])
]

var bestResult: MultivariateOptimizationResult<VectorN<Double>>?
var bestValue = Double.infinity

for start in startingPoints {
    let result = try optimizer.minimize(objective, from: start)
    if result.value < bestValue {
        bestValue = result.value
        bestResult = result
    }
}
```

### Adaptive Configuration

```swift
// Start with fast config for initial search
let fastResult = try NelderMead<VectorN<Double>>(config: .fast)
    .minimize(objective, from: initialGuess)

// Refine with high precision config
let finalResult = try NelderMead<VectorN<Double>>(config: .highPrecision)
    .minimize(objective, from: fastResult.solution)
```

### Monitoring Convergence

```swift
let result = optimizer.optimizeDetailed(objective: f, initialGuess: x0)

if let history = result.convergenceHistory {
    // Plot or analyze convergence
    for (i, value) in history.enumerated() {
        if i % 50 == 0 {
            print("Iteration \(i): value = \(value)")
        }
    }
}
```

## Next Steps

- Review <doc:5.20-LBFGSOptimizationTutorial> for fast gradient-based optimization
- Try <doc:5.21-ConjugateGradientTutorial> for another gradient method
- Explore <doc:5.22-SimulatedAnnealingTutorial> for global optimization
- Learn about <doc:5.5-MultivariateOptimization> for multivariate theory

## See Also

- ``NelderMead``
- ``NelderMeadConfig``
- ``NelderMeadResult``
- ``MultivariateOptimizer``
- ``MultivariateConstraint``
- ``MultivariateOptimizationResult``
- ``VectorSpace``
- ``VectorN``
