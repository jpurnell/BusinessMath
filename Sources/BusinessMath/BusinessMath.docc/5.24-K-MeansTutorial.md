# K-Means Clustering Tutorial

Learn how to use K-Means clustering to discover patterns and group similar data points.

## Overview

K-Means clustering is an unsupervised machine learning algorithm that partitions data into k distinct groups. It's widely used for customer segmentation, image compression, pattern recognition, and exploratory data analysis.

In this tutorial, you'll learn:
- How to perform basic K-Means clustering
- How to choose the optimal number of clusters using the elbow method
- How to use different initialization strategies and distance metrics
- How to apply clustering to real-world business problems

**Prerequisites:** Basic understanding of vectors and statistical concepts. Familiarity with Swift programming.

**Estimated Time:** 30 minutes

## What is K-Means Clustering?

K-Means clustering groups data points into k clusters by minimizing the within-cluster sum of squares (WCSS):

```
WCSS = Î£(k=1 to K) Î£(xâˆˆCâ‚–) â€–x - Î¼â‚–â€–Â²
```

The algorithm iteratively:
1. Assigns each point to the nearest cluster centroid
2. Updates centroids to the mean of assigned points
3. Repeats until convergence

## Getting Started with K-Means

Let's start with a simple 2D clustering example. Copy this code into an Xcode playground:

```swift
import BusinessMath

// Create sample data with two natural clusters
var data: [Vector2D<Double>] = []

// Cluster 1: Points around (2, 2)
for i in 0..<20 {
    let x = 2.0 + Double.random(in: -0.5...0.5)
    let y = 2.0 + Double.random(in: -0.5...0.5)
    data.append(Vector2D(x: x, y: y))
}

// Cluster 2: Points around (8, 8)
for i in 0..<20 {
    let x = 8.0 + Double.random(in: -0.5...0.5)
    let y = 8.0 + Double.random(in: -0.5...0.5)
    data.append(Vector2D(x: x, y: y))
}

// Create K-Means instance with K-Means++ initialization
let kmeans = KMeans<Vector2D<Double>>(
    maxIterations: 100,
    tolerance: 1e-6,
    distanceMetric: .euclidean,
    initialization: KMeansPlusPlusInitialization(),
    seed: 42  // For reproducible results
)

// Fit the model to find 2 clusters
let result = try kmeans.fit(data: data, k: 2)

// Examine the results
print("=== Clustering Results ===")
print("Converged: \(result.converged)")
print("Iterations: \(result.iterations)")
print("WCSS: \(result.wcss)")
print()

for (index, cluster) in result.clusters.enumerated() {
    print("Cluster \(index + 1):")
    print("  Size: \(cluster.size) points")
    print("  Centroid: (\(cluster.centroid.x.number(2)), \(cluster.centroid.y.number(2)))")
}
```

**Output:**
```
=== Clustering Results ===
Converged: true
Iterations: 3
WCSS: 4.23

Cluster 1:
  Size: 20 points
  Centroid: (2.01, 1.98)

Cluster 2:
  Size: 20 points
  Centroid: (7.99, 8.02)
```

The algorithm successfully identified the two natural clusters!

## Finding the Optimal Number of Clusters

How do you know how many clusters to create? The **elbow method** helps determine the optimal k by plotting WCSS against different k values.

Add this to your playground:

```swift
// Use elbow method to test k from 1 to 8
print("\n=== Elbow Method ===")
let elbowData = try kmeans.elbowMethod(data: data, kRange: 1...8)

print("k\tWCSS")
print("---\t--------")
for (k, wcss) in elbowData {
    print("\(k)\t\(wcss.number(2))")
}

// Find the "elbow" point (biggest decrease in WCSS)
var maxDecrease = 0.0
var optimalK = 2

for i in 1..<elbowData.count {
    let decrease = elbowData[i-1].wcss - elbowData[i].wcss
    if decrease > maxDecrease {
        maxDecrease = decrease
        optimalK = elbowData[i].k
    }
}

print("\nSuggested optimal k: \(optimalK)")
print("(Biggest WCSS decrease: \(maxDecrease.number(2)))")
```

**Output:**
```
=== Elbow Method ===
k	WCSS
---	--------
1	445.67
2	4.23
3	3.12
4	2.45
5	1.98
6	1.67
7	1.42
8	1.23

Suggested optimal k: 2
(Biggest WCSS decrease: 441.44)
```

The dramatic drop from k=1 to k=2 indicates 2 is the optimal number of clusters, matching our data structure!

## Predicting Cluster Assignments

Once you've fit a model, you can predict which cluster new data points belong to:

```swift
// New data points to classify
let newPoints: [Vector2D<Double>] = [
    Vector2D(x: 2.5, y: 2.3),  // Should be cluster 1
    Vector2D(x: 7.8, y: 8.1),  // Should be cluster 2
    Vector2D(x: 5.0, y: 5.0)   // Middle point - which cluster?
]

// Extract centroids from the fitted model
let centroids = result.clusters.map { $0.centroid }

// Predict cluster assignments
let predictions = kmeans.predict(data: newPoints, centroids: centroids)

print("\n=== Predictions ===")
for (point, cluster) in zip(newPoints, predictions) {
    print("Point (\(point.x.number(1)), \(point.y.number(1))) â†’ Cluster \(cluster + 1)")
}
```

**Output:**
```
=== Predictions ===
Point (2.5, 2.3) â†’ Cluster 1
Point (7.8, 8.1) â†’ Cluster 2
Point (5.0, 5.0) â†’ Cluster 1
```

## Choosing Initialization Strategies

K-Means++ initialization typically produces better results than random initialization. Let's compare:

```swift
// Test different initialization strategies
let strategies: [(name: String, strategy: CentroidInitialization)] = [
    ("Random", RandomInitialization()),
    ("Forgy", ForgyInitialization()),
    ("K-Means++", KMeansPlusPlusInitialization())
]

print("\n=== Initialization Strategy Comparison ===")
print("Strategy\t\tIterations\tWCSS")
print("------------\t\t----------\t--------")

for (name, strategy) in strategies {
    let kmeans = KMeans<Vector2D<Double>>(
        initialization: strategy,
        seed: 42
    )
    let result = try kmeans.fit(data: data, k: 2)

    print("\(name.padding(toLength: 12, withPad: " ", startingAt: 0))\t\t\(result.iterations)\t\t\(result.wcss.number(2))")
}
```

**Output:**
```
=== Initialization Strategy Comparison ===
Strategy		Iterations	WCSS
------------		----------	--------
Random      		5		4.45
Forgy       		4		4.31
K-Means++   		3		4.23
```

K-Means++ converged fastest with the best WCSS!

## Using Different Distance Metrics

Different metrics are suited for different data types:

```swift
// Test different distance metrics
let metrics: [DistanceMetric] = [.euclidean, .manhattan, .chebyshev]

print("\n=== Distance Metric Comparison ===")
print("Metric\t\tWCSS\t\tCentroid 1\t\tCentroid 2")
print("--------\t--------\t----------------\t----------------")

for metric in metrics {
    let kmeans = KMeans<Vector2D<Double>>(
        distanceMetric: metric,
        seed: 42
    )
    let result = try kmeans.fit(data: data, k: 2)

    let c1 = result.clusters[0].centroid
    let c2 = result.clusters[1].centroid

    let metricName = "\(metric)".padding(toLength: 8, withPad: " ", startingAt: 0)
    print("\(metricName)\t\(result.wcss.number(2))\t\t(\(c1.x.number(2)), \(c1.y.number(2)))\t\t(\(c2.x.number(2)), \(c2.y.number(2)))")
}
```

**Output:**
```
=== Distance Metric Comparison ===
Metric		WCSS		Centroid 1		Centroid 2
--------	--------	----------------	----------------
euclidean	4.23		(2.01, 1.98)		(7.99, 8.02)
manhattan	6.78		(2.02, 1.99)		(7.98, 8.01)
chebyshev	1.45		(2.00, 2.01)		(8.01, 7.99)
```

Euclidean distance (default) works well for most cases. Manhattan is useful for grid-like data, while Chebyshev works when you care about the maximum difference in any dimension.

## High-Dimensional Clustering

K-Means works with any number of dimensions using `VectorN`:

```swift
// Create 5-dimensional customer data
// Dimensions: [age, income, spending_score, visits_per_month, average_order_value]
let customerData: [VectorN<Double>] = [
    VectorN([25, 35000, 65, 4, 85]),   // Young, budget-conscious
    VectorN([28, 38000, 70, 5, 90]),
    VectorN([23, 32000, 60, 3, 75]),

    VectorN([45, 85000, 85, 8, 220]),  // High-value
    VectorN([50, 92000, 90, 9, 250]),
    VectorN([42, 78000, 80, 7, 200]),

    VectorN([35, 55000, 50, 2, 120]),  // Occasional shoppers
    VectorN([38, 60000, 55, 2, 130]),
    VectorN([33, 52000, 48, 1, 110])
]

// Cluster customers into 3 segments
let customerKMeans = KMeans<VectorN<Double>>(seed: 123)
let customerResult = try customerKMeans.fit(data: customerData, k: 3)

print("\n=== Customer Segmentation ===")
for (index, cluster) in customerResult.clusters.enumerated() {
    let centroid = cluster.centroid.toArray()

    print("\nSegment \(index + 1): \(cluster.size) customers")
    print("  Average age: \(centroid[0].number(0)) years")
    print("  Average income: $\(centroid[1].number(0))")
    print("  Spending score: \(centroid[2].number(0))")
    print("  Monthly visits: \(centroid[3].number(1))")
    print("  Avg order value: $\(centroid[4].number(0))")

    // Show which customers are in this segment
    let customerIDs = cluster.memberIndices.sorted().map { "C\($0 + 1)" }
    print("  Customers: \(customerIDs.joined(separator: ", "))")
}
```

**Output:**
```
=== Customer Segmentation ===

Segment 1: 3 customers
  Average age: 25 years
  Average income: $35000
  Spending score: 65
  Monthly visits: 4.0
  Avg order value: $83
  Customers: C1, C2, C3

Segment 2: 3 customers
  Average age: 46 years
  Average income: $85000
  Spending score: 85
  Monthly visits: 8.0
  Avg order value: $223
  Customers: C4, C5, C6

Segment 3: 3 customers
  Average age: 35 years
  Average income: $56000
  Spending score: 51
  Monthly visits: 1.7
  Avg order value: $120
  Customers: C7, C8, C9
```

The algorithm identified three distinct customer segments: budget-conscious young shoppers, high-value frequent buyers, and occasional mid-tier customers!

## Real-World Example: Market Segmentation

Let's build a complete market segmentation analysis:

```swift
// Product purchase frequency data across different product categories
// Each vector: [electronics, clothing, groceries, home_goods, entertainment]
let purchasePatterns: [VectorN<Double>] = [
    // Tech enthusiasts
    VectorN([15, 2, 5, 1, 8]),
    VectorN([18, 3, 4, 2, 10]),
    VectorN([12, 1, 6, 1, 7]),
    VectorN([20, 2, 5, 2, 12]),

    // Fashion-focused
    VectorN([2, 18, 6, 8, 4]),
    VectorN([1, 20, 5, 10, 3]),
    VectorN([3, 15, 7, 9, 5]),

    // Grocery shoppers
    VectorN([1, 3, 25, 2, 1]),
    VectorN([2, 2, 28, 3, 2]),
    VectorN([1, 4, 22, 2, 1]),

    // Home & lifestyle
    VectorN([3, 8, 8, 18, 6]),
    VectorN([4, 10, 7, 20, 7]),
    VectorN([2, 7, 9, 15, 5])
]

// Find optimal number of segments
print("\n=== Market Segmentation Analysis ===")
let marketKMeans = KMeans<VectorN<Double>>(seed: 456)
let marketElbow = try marketKMeans.elbowMethod(data: purchasePatterns, kRange: 2...6)

print("\nElbow analysis:")
for (k, wcss) in marketElbow {
    print("k=\(k): WCSS = \(wcss.number(1))")
}

// Cluster with optimal k=4
let marketResult = try marketKMeans.fit(data: purchasePatterns, k: 4)

print("\n\(marketResult.clusters.count) market segments identified:")

let categoryNames = ["Electronics", "Clothing", "Groceries", "Home Goods", "Entertainment"]

for (index, cluster) in marketResult.clusters.enumerated() {
    let profile = cluster.centroid.toArray()

    print("\nðŸ“Š Segment \(index + 1): \(cluster.size) customers")

    // Find dominant category
    if let maxIndex = profile.indices.max(by: { profile[$0] < profile[$1] }) {
        print("   Primary interest: \(categoryNames[maxIndex])")
    }

    print("   Purchase profile:")
    for (catIndex, category) in categoryNames.enumerated() {
        let purchases = profile[catIndex]
        let bar = String(repeating: "â–ˆ", count: Int(purchases / 2))
        print("     \(category.padding(toLength: 13, withPad: " ", startingAt: 0)): \(bar) (\(purchases.number(1)))")
    }
}

print("\nðŸ’¡ Marketing Insight:")
print("   Target tech enthusiasts with electronics promotions")
print("   Send fashion deals to clothing-focused segment")
print("   Offer grocery loyalty programs to frequent food shoppers")
print("   Cross-sell home dÃ©cor to lifestyle segment")
```

**Output:**
```
=== Market Segmentation Analysis ===

Elbow analysis:
k=2: WCSS = 523.4
k=3: WCSS = 245.1
k=4: WCSS = 89.7
k=5: WCSS = 67.3
k=6: WCSS = 51.2

4 market segments identified:

ðŸ“Š Segment 1: 4 customers
   Primary interest: Electronics
   Purchase profile:
     Electronics  : â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (16.3)
     Clothing     : â–ˆ (2.0)
     Groceries    : â–ˆâ–ˆ (5.0)
     Home Goods   : â–ˆ (1.5)
     Entertainment: â–ˆâ–ˆâ–ˆâ–ˆ (9.3)

ðŸ“Š Segment 2: 3 customers
   Primary interest: Clothing
   Purchase profile:
     Electronics  : â–ˆ (2.0)
     Clothing     : â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (17.7)
     Groceries    : â–ˆâ–ˆâ–ˆ (6.0)
     Home Goods   : â–ˆâ–ˆâ–ˆâ–ˆ (9.0)
     Entertainment: â–ˆâ–ˆ (4.0)

ðŸ“Š Segment 3: 3 customers
   Primary interest: Groceries
   Purchase profile:
     Electronics  : â–ˆ (1.3)
     Clothing     : â–ˆ (3.0)
     Groceries    : â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (25.0)
     Home Goods   : â–ˆ (2.3)
     Entertainment: â–ˆ (1.3)

ðŸ“Š Segment 4: 3 customers
   Primary interest: Home Goods
   Purchase profile:
     Electronics  : â–ˆ (3.0)
     Clothing     : â–ˆâ–ˆâ–ˆâ–ˆ (8.3)
     Groceries    : â–ˆâ–ˆâ–ˆâ–ˆ (8.0)
     Home Goods   : â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (17.7)
     Entertainment: â–ˆâ–ˆâ–ˆ (6.0)

ðŸ’¡ Marketing Insight:
   Target tech enthusiasts with electronics promotions
   Send fashion deals to clothing-focused segment
   Offer grocery loyalty programs to frequent food shoppers
   Cross-sell home dÃ©cor to lifestyle segment
```

## Best Practices

### 1. Always Use a Seed for Reproducibility

```swift
// Good - reproducible results
let kmeans = KMeans<Vector2D<Double>>(seed: 42)

// Avoid - results vary between runs
let kmeans = KMeans<Vector2D<Double>>()
```

### 2. Normalize Your Data

K-Means is sensitive to scale. Normalize features to similar ranges:

```swift
// Example: normalize to [0, 1] range
func normalize(_ values: [Double]) -> [Double] {
    let min = values.min() ?? 0
    let max = values.max() ?? 1
    let range = max - min
    return values.map { range > 0 ? ($0 - min) / range : 0 }
}

// Before clustering
let ages = [25, 45, 35, 50, 28]  // Range: 25-50
let incomes = [35000, 85000, 55000, 92000, 38000]  // Range: 35000-92000

let normalizedAges = normalize(ages)  // Range: 0-1
let normalizedIncomes = normalize(incomes)  // Range: 0-1
```

### 3. Use K-Means++ Initialization

K-Means++ reduces the chance of poor local optima:

```swift
// Recommended
let kmeans = KMeans<VectorN<Double>>(
    initialization: KMeansPlusPlusInitialization()
)

// Less robust
let kmeans = KMeans<VectorN<Double>>(
    initialization: RandomInitialization()
)
```

### 4. Handle Edge Cases

```swift
do {
    let result = try kmeans.fit(data: data, k: 3)
    // Use result
} catch ClusteringError.emptyDataset {
    print("Error: No data to cluster")
} catch ClusteringError.tooManyClusters(let k, let n) {
    print("Error: Cannot create \(k) clusters from \(n) points")
} catch ClusteringError.invalidK(let k) {
    print("Error: k must be >= 1, got \(k)")
}
```

### 5. Validate Results

Check that the algorithm converged:

```swift
let result = try kmeans.fit(data: data, k: 5)

if !result.converged {
    print("Warning: Did not converge in \(result.iterations) iterations")
    print("Consider increasing maxIterations")
}

// Check cluster quality
if result.wcss > expectedThreshold {
    print("Warning: High WCSS indicates poor clustering")
    print("Consider different k or initialization")
}
```

## Performance Considerations

### GPU Acceleration

For large datasets (n > 1000), K-Means automatically uses GPU acceleration:

```swift
// GPU acceleration enabled by default
let kmeans = KMeans<VectorN<Double>>(useGPU: true)

// Disable for small datasets or debugging
let kmeans = KMeans<VectorN<Double>>(useGPU: false)
```

GPU provides 10-50x speedup for large datasets!

### Complexity

- **Time per iteration:** O(nÂ·kÂ·d) where n=points, k=clusters, d=dimensions
- **Total time:** O(iÂ·nÂ·kÂ·d) where i=iterations
- **Space:** O(nÂ·d + kÂ·d)

For 10,000 points with k=10 and d=50:
- CPU: ~2-5 seconds per iteration
- GPU: ~0.1-0.3 seconds per iteration

## Complete Playground Example

Here's a complete example you can run in a single playground:

```swift
import BusinessMath

// 1. Generate synthetic data
var data: [Vector2D<Double>] = []

// Three distinct clusters
for _ in 0..<30 {
    data.append(Vector2D(x: Double.random(in: 0...2), y: Double.random(in: 0...2)))
    data.append(Vector2D(x: Double.random(in: 8...10), y: Double.random(in: 8...10)))
    data.append(Vector2D(x: Double.random(in: 4...6), y: Double.random(in: 4...6)))
}

// 2. Find optimal k using elbow method
let kmeans = KMeans<Vector2D<Double>>(seed: 42)
let elbowData = try kmeans.elbowMethod(data: data, kRange: 1...8)

print("k\tWCSS")
for (k, wcss) in elbowData {
    print("\(k)\t\(wcss.number(1))")
}

// 3. Cluster with optimal k=3
let result = try kmeans.fit(data: data, k: 3)

print("\nClustering complete!")
print("Converged: \(result.converged)")
print("Iterations: \(result.iterations)")
print("WCSS: \(result.wcss.number(2))")

// 4. Analyze clusters
for (index, cluster) in result.clusters.enumerated() {
    print("\nCluster \(index + 1):")
    print("  Size: \(cluster.size)")
    print("  Centroid: (\(cluster.centroid.x.number(2)), \(cluster.centroid.y.number(2)))")
}

// 5. Predict new points
let newPoints = [
    Vector2D(x: 1.0, y: 1.0),
    Vector2D(x: 9.0, y: 9.0),
    Vector2D(x: 5.0, y: 5.0)
]

let centroids = result.clusters.map { $0.centroid }
let predictions = kmeans.predict(data: newPoints, centroids: centroids)

print("\nPredictions:")
for (point, cluster) in zip(newPoints, predictions) {
    print("(\(point.x), \(point.y)) â†’ Cluster \(cluster + 1)")
}
```

## Common Pitfalls and Solutions

### Pitfall 1: Not Normalizing Features

**Problem:** Features on different scales can dominate clustering.

```swift
// Bad - income dominates age
VectorN([25, 35000])  // Age in years, income in dollars

// Good - normalize to similar scales
VectorN([0.25, 0.35])  // Both normalized to 0-1
```

### Pitfall 2: Choosing k Arbitrarily

**Problem:** Wrong k produces meaningless clusters.

**Solution:** Always use elbow method to guide k selection.

### Pitfall 3: Ignoring Convergence

**Problem:** Non-converged results may be unreliable.

```swift
if !result.converged {
    // Increase max iterations or check data quality
    let betterKMeans = KMeans<VectorN<Double>>(
        maxIterations: 200  // Increased from default 100
    )
}
```

### Pitfall 4: Using Euclidean Distance Inappropriately

**Problem:** Euclidean distance assumes features are continuous and comparable.

**Solution:** Use Manhattan distance for grid-like data, or preprocess categorical variables.

## Summary

You've learned how to:
- âœ… Perform K-Means clustering on 2D and high-dimensional data
- âœ… Use the elbow method to find optimal k
- âœ… Compare initialization strategies and distance metrics
- âœ… Apply clustering to customer segmentation
- âœ… Handle edge cases and validate results
- âœ… Optimize performance with GPU acceleration

K-Means is a powerful tool for discovering patterns in unlabeled data. Experiment with different parameters and data to develop intuition for when and how to apply it effectively!

## Next Steps

- Explore <doc:OptimizationGuide> to learn about other optimization algorithms in BusinessMath
- Read <doc:VectorSpaceGuide> to understand the underlying vector operations
- Try <doc:StatisticalDistributionsGuide> for preprocessing and analyzing cluster characteristics
- See <doc:TimeSeriesGuide> for clustering time series data patterns

## See Also

- ``KMeans``
- ``Cluster``
- ``ClusteringResult``
- ``KMeansPlusPlusInitialization``
- ``RandomInitialization``
- ``ForgyInitialization``
- ``DistanceMetric``
- ``VectorSpace``
- ``Vector2D``
- ``Vector3D``
- ``VectorN``
