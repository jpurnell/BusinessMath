# VectorSpace Foundation

Introducing the `VectorSpace` protocol and related infrastructure.

---

## Overview

This tutorial introduces the `VectorSpace` protocol and related infrastructure, which enables BusinessMath to work with vectors of any dimension in a type-safe, generic way.

**Key Components:**
1. **VectorSpace Protocol** - Generic interface for vector operations
2. **Vector Implementations** - Vector2D, Vector3D, VectorN
3. **MultivariateConstraint** - Type-safe constraint specification
4. **Extended Operations** - Distance metrics, projections, transformations

This foundation powers all multivariate optimization.

---

## 1. The VectorSpace Protocol

### What is a Vector Space?

A **vector space** is a mathematical structure supporting:
- **Vector addition**: v + w
- **Scalar multiplication**: Œ± ¬∑ v
- **Zero element**: 0
- **Norms and distances**: ‚Äñv‚Äñ, ‚Äñv - w‚Äñ

### Protocol Definition

```swift
public protocol VectorSpace: AdditiveArithmetic, Hashable, Codable, Sendable {
    associatedtype Scalar: Real & Sendable & Codable

    // Required operations
    static var zero: Self { get }
    static func + (lhs: Self, rhs: Self) -> Self
    static func * (lhs: Scalar, rhs: Self) -> Self
    static prefix func - (vector: Self) -> Self

    // Norm and distance
    var norm: Scalar { get }
    func dot(_ other: Self) -> Scalar

    // Conversion
    static func fromArray(_ array: [Scalar]) -> Self?
    func toArray() -> [Scalar]

    // Dimension
    static var dimension: Int { get }
    var isFinite: Bool { get }
}
```

### Why VectorSpace?

**Problem:** Different vector types (2D, 3D, N-dimensional) had duplicate code

**Solution:** Single protocol + generic algorithms work for all vectors

```swift
// ‚ùå Before Phase 2: Duplicate implementations
func optimize2D(_ f: (Vector2D) -> Double, ...) -> Vector2D
func optimize3D(_ f: (Vector3D) -> Double, ...) -> Vector3D
func optimizeND(_ f: (VectorN) -> Double, ...) -> VectorN

// ‚úÖ After Phase 2: One generic implementation
func optimize<V: VectorSpace>(_ f: (V) -> V.Scalar, ...) -> V
```

---

## 2. Vector Implementations

### Vector2D - Fixed 2D Vectors

**Use Cases:**
- 2D coordinate systems
- Complex numbers (x = real, y = imaginary)
- Two-variable optimization
- Graphics and geometry

**Performance:** Fastest (compile-time optimization, no array overhead)

```swift
import BusinessMath

// Create a 2D vector
let v = Vector2D<Double>(x: 3.0, y: 4.0)

// Basic operations
let w = Vector2D(x: 1.0, y: 2.0)
let sum = v + w                    // Vector2D(x: 4.0, y: 6.0)
let scaled = 2.0 * v               // Vector2D(x: 6.0, y: 8.0)

// Norm and distance
print(v.norm)                      // 5.0 (‚àö(3¬≤ + 4¬≤))
print(v.distance(to: w))           // 2.828...

// Dot product
print(v.dot(w))                    // 11.0 (3*1 + 4*2)

// 2D-specific operations
print(v.cross(w))                  // 2.0 (pseudo-cross product)
print(v.angle)                     // 0.927... radians
let rotated = v.rotated(by: .pi/2) // Vector2D(x: -4.0, y: 3.0)
```

### Vector3D - Fixed 3D Vectors

**Use Cases:**
- 3D coordinate systems
- RGB color spaces
- Three-variable optimization
- Cross product calculations

**Performance:** Very fast (compile-time optimization)

```swift
import BusinessMath

// Create a 3D vector
let v3 = Vector3D<Double>(x: 1.0, y: 2.0, z: 3.0)
let w3 = Vector3D<Double>(x: 4.0, y: 5.0, z: 6.0)

// Basic operations
let sum3 = v3 + w3               // Vector3D(x: 5.0, y: 7.0, z: 9.0)
let scaled3 = 2.0 * v3          // Vector3D(x: 2.0, y: 4.0, z: 6.0)

// Norm and dot product
print(v3.norm)                 // 3.742... (‚àö(1¬≤ + 2¬≤ + 3¬≤))
print(v3.dot(w3))               // 32.0 (1*4 + 2*5 + 3*6)

// 3D-specific operations
let cross = v3.cross(w3)        // Vector3D perpendicular to both
print(cross)                  // Vector3D(x: -3.0, y: 6.0, z: -3.0)

// Verify perpendicularity
print(v3.dot(cross))           // ~0.0
print(w3.dot(cross))           // ~0.0

// Triple products
let tripleScalar = v3.tripleProduct(w3, v3)  // Signed volume
let tripleVector = v3.vectorTripleProduct(w3, v3)
```

### VectorN - Variable N-Dimensional Vectors

**Use Cases:**
- High-dimensional optimization
- Machine learning feature vectors
- Portfolio weights (N assets)
- Any variable or large dimension problem

**Performance:** Flexible but has array bounds checking overhead

```swift
import BusinessMath

// Create an N-dimensional vector
let vN = VectorN<Double>([1.0, 2.0, 3.0, 4.0, 5.0])

// Basic operations
let wN = VectorN([5.0, 4.0, 3.0, 2.0, 1.0])
let sumN = vN + wN               // VectorN([6, 6, 6, 6, 6])
let scaledN = 2.0 * vN          // VectorN([2, 4, 6, 8, 10])

// Norm and dot product
print(vN.norm)                 // 7.416... (‚àö55)
print(vN.dot(wN))               // 35.0

// Element access
print(vN[0])                   // 1.0
print(vN[2])                   // 3.0

// Dimension
print(vN.dimension)            // 5
print(vN.count)                // 5

// Statistical operations
print(vN.sum)                  // 15.0
print(vN.mean)                 // 3.0
print(vN.standardDeviation())  // 1.581...
print(vN.min)                  // 1.0
print(vN.max)                  // 5.0

// Element-wise operations
let product = vN.hadamardProduct(with: wN)  // [5, 8, 9, 8, 5]
let quotient = vN.elementwiseDivide(by: wN) // [0.2, 0.5, 1.0, 2.0, 5.0]
print(product)
print(quotient)
```

---

## 3. Common Operations

All vector types share these operations through the `VectorSpace` protocol:

### Arithmetic Operations

```swift
let v = VectorN([1.0, 2.0, 3.0])
let w = VectorN([4.0, 5.0, 6.0])

// Addition and subtraction
let sum = v + w               // [5, 7, 9]
let diff = v - w              // [-3, -3, -3]

// Scalar multiplication
let scaled = 3.0 * v          // [3, 6, 9]
let divided = v / 2.0         // [0.5, 1.0, 1.5]

// Negation
let negated = -v              // [-1, -2, -3]
```

### Norms and Distances

```swift
let v = VectorN([3.0, 4.0])

// Norms
print(v.norm)                 // 5.0 (Euclidean: ‚àö(3¬≤ + 4¬≤))
print(v.squaredNorm)          // 25.0 (faster for comparisons)

// Distance metrics
let w = VectorN([0.0, 0.0])
print(v.distance(to: w))      // 5.0 (Euclidean distance)
print(v.manhattanDistance(to: w))    // 7.0 (|3| + |4|)
print(v.chebyshevDistance(to: w))    // 4.0 (max(|3|, |4|))
```

### Dot Products and Angles

```swift
let v = VectorN([1.0, 0.0, 0.0])
let w = VectorN([0.0, 1.0, 0.0])

// Dot product
print(v.dot(w))               // 0.0 (perpendicular)

// Cosine similarity
print(v.cosineSimilarity(with: w))  // 0.0 (orthogonal)

// Angle between vectors
let angle = v.angle(with: w)  // œÄ/2 radians (90 degrees)
```

### Projections

```swift
let v = VectorN([3.0, 4.0])
let w = VectorN([1.0, 0.0])

// Project v onto w
let projection = v.projection(onto: w)  // [3.0, 0.0]

// Rejection (component perpendicular to w)
let rejection = v.rejection(from: w)    // [0.0, 4.0]

// Verify: v = projection + rejection
assert(v == projection + rejection)
```

### Normalization

```swift
let v = VectorN([3.0, 4.0])

// Normalize to unit length
let unit = v.normalized()     // [0.6, 0.8]
print(unit.norm)              // 1.0

// Verify direction preserved
print(v.cosineSimilarity(with: unit))  // 1.0 (same direction)
```

---

## 4. VectorN-Specific Operations

### Construction

```swift
// From array
let v1 = VectorN([1.0, 2.0, 3.0])

// Repeating value
let v2 = VectorN(repeating: 5.0, count: 10)

// Zero vector
let v3 = VectorN<Double>.zero

// Ones vector
let v4 = VectorN<Double>.ones(dimension: 5)

// Basis vector (one component = 1, rest = 0)
let e2 = VectorN<Double>.basisVector(dimension: 5, index: 2)
// [0, 0, 1, 0, 0]

// Linear space (evenly spaced)
let v5 = VectorN.linearSpace(from: 0.0, to: 10.0, count: 11)
// [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

// Log space (logarithmically spaced)
let v6 = VectorN.logSpace(from: 1.0, to: 100.0, count: 3)
// [1, 10, 100]
```

### Manipulation

```swift
var v = VectorN([1.0, 2.0, 3.0])

// Access elements
print(v[0])                   // 1.0
v[1] = 5.0                    // v = [1, 5, 3]

// Append/remove
let v2 = v.appending(4.0)     // [1, 5, 3, 4]
let v3 = v2.removingLast()    // [1, 5, 3]

// Concatenate
let w = VectorN([6.0, 7.0])
let combined = v.concatenated(with: w)  // [1, 5, 3, 6, 7]

// Slice
let slice = combined.slice(1..<4)  // [5, 3, 6]
```

### Functional Operations

```swift
let v = VectorN([-2.0, -1.0, 0.0, 1.0, 2.0, 3.0])

// Map (element-wise transform)
let squared = v.map { $0 * $0 }  // [4, 1, 0, 1, 4, 9]

// Filter
let positive = v.filter { $0 > 0 }  // [1, 2, 3]

// Reduce
let sum = v.reduce(0.0, +)  // 3.0

// Zip with another vector
let w = VectorN([4.0, 5.0, 6.0, 7.0, 8.0, 9.0])
let product = v.zipWith(w, *)  // [-8.0, -5.0, 0.0, 7.0, 16.0, 27.0]
```

### Orthogonality and Parallelism

```swift
let v = VectorN([1.0, 0.0, 0.0])
let w = VectorN([0.0, 1.0, 0.0])
let u = VectorN([2.0, 0.0, 0.0])

// Check orthogonality
print(v.isOrthogonal(to: w))  // true
print(v.isOrthogonal(to: u))  // false

// Check parallelism
print(v.isParallel(to: u))    // true (same direction)
print(v.isParallel(to: w))    // false
```

---

## 5. MultivariateConstraint

Phase 2 introduced type-safe constraint specification for multivariate problems:

### Constraint Types

```swift
enum MultivariateConstraint<V: VectorSpace> {
    case equality(function: (V) -> V.Scalar, gradient: ((V) -> V)?)
    case inequality(function: (V) -> V.Scalar, gradient: ((V) -> V)?)
}
```

### Equality Constraints (h(x) = 0)

Constraints that must be exactly satisfied (e.g., budget must be exactly 100% allocated).

```swift
import BusinessMath

// Example: Portfolio allocation where weights MUST sum to exactly 100%
// If we invest in 3 assets, their weights must sum to 1.0 (100%)

// Define the constraint: sum(weights) - 1.0 = 0
let budgetConstraint: MultivariateConstraint<VectorN<Double>> = .equality { weights in
    // This function returns 0 when weights sum to 1.0
    // For example: if weights = [0.3, 0.4, 0.3], sum = 1.0, result = 0 ‚úì
    // If weights = [0.2, 0.5, 0.2], sum = 0.9, result = -0.1 ‚úó
    weights.reduce(0, +) - 1.0
}

// Use the constraint in optimization
// (This is conceptual - actual optimizer setup shown in later phases)
/*
let result = try optimizer.minimize(
    portfolioRisk,  // Minimize risk
    from: VectorN([0.33, 0.33, 0.34]),  // Starting guess
    subjectTo: [budgetConstraint]  // Must be fully invested
)
*/
```

### Inequality Constraints (g(x) ‚â§ 0)

Constraints that set bounds or limits (e.g., no negative investments, maximum allocation).

```swift
import BusinessMath

// Example 1: No short-selling constraint
// We want: weight[0] ‚â• 0 (can't invest negative amount)
// Convert to standard form (‚â§ 0): -weight[0] ‚â§ 0
let noShortSelling: MultivariateConstraint<VectorN<Double>> = .inequality { weights in
    // This returns a negative number when weight[0] > 0 (valid)
    // and a positive number when weight[0] < 0 (invalid)
    -weights[0]
}

// Example 2: Maximum allocation constraint
// We want: weight[1] ‚â§ 50% (can't put more than half in one asset)
// Convert to standard form: weight[1] - 0.5 ‚â§ 0
let maxAllocation: MultivariateConstraint<VectorN<Double>> = .inequality { weights in
    // This returns negative when weight[1] < 0.5 (valid)
    // and positive when weight[1] > 0.5 (invalid)
    weights[1] - 0.5
}

// Example 3: Minimum allocation constraint
// We want: weight[2] ‚â• 10% (must invest at least 10% in bonds for safety)
// Convert to standard form: -weight[2] + 0.1 ‚â§ 0, or 0.1 - weight[2] ‚â§ 0
let minBonds: MultivariateConstraint<VectorN<Double>> = .inequality { weights in
    0.1 - weights[2]
}

// Test the constraints
let testWeights = VectorN([0.3, 0.4, 0.3])  // 30% / 40% / 30%

print("Testing weights: \(testWeights.toArray())")
print("No short-selling check (‚â§ 0): \(-testWeights[0])")  // -0.3 ‚úì (satisfied)
print("Max allocation check (‚â§ 0): \(testWeights[1] - 0.5)")  // -0.1 ‚úì (satisfied)
print("Min bonds check (‚â§ 0): \(0.1 - testWeights[2])")  // -0.2 ‚úì (satisfied)
```

### With Analytical Gradients

Provide gradients for better performance. Gradients tell the optimizer which direction to move to satisfy constraints faster.

```swift
import BusinessMath

// Budget constraint with gradient: sum(weights) - 1.0 = 0
// The gradient tells us: increasing any weight by 1 unit increases the sum by 1
let budgetConstraint: MultivariateConstraint<VectorN<Double>> = .equality(
    function: { weights in
        // Constraint function: returns 0 when satisfied
        weights.sum - 1.0
    },
    gradient: { weights in
        // Gradient: ‚àá(sum(w) - 1) = [1, 1, 1, ...]
        // This says: each weight contributes equally to the sum
        VectorN(repeating: 1.0, count: weights.dimension)
    }
)

// Factory function: Create non-negativity constraints with gradients
// This is more efficient than numerical differentiation
func createNoShortSellingConstraint(for assetIndex: Int, totalAssets: Int) -> MultivariateConstraint<VectorN<Double>> {
    .inequality(
        function: { weights in
            // Constraint: -weight[i] ‚â§ 0, which means weight[i] ‚â• 0
            -weights[assetIndex]
        },
        gradient: { weights in
            // Gradient: ‚àá(-w[i]) = unit vector pointing in direction i
            // Only the i-th component is non-zero
            -VectorN<Double>.basisVector(dimension: totalAssets, index: assetIndex)
        }
    )
}

// Example: Create constraints for a 3-asset portfolio
let noShortAsset0 = createNoShortSellingConstraint(for: 0, totalAssets: 3)
let noShortAsset1 = createNoShortSellingConstraint(for: 1, totalAssets: 3)
let noShortAsset2 = createNoShortSellingConstraint(for: 2, totalAssets: 3)

print("Created 3 no-short-selling constraints with analytical gradients")
print("Optimizer will converge faster with analytical gradients vs numerical approximation")
```

---

## 6. Scalar as VectorSpace

Even scalars (Double, Float) conform to VectorSpace:

```swift
// Double is a 1D vector space
let x: Double = 5.0

// VectorSpace operations
print(Double.zero)            // 0.0
print(x.norm)                 // 5.0 (absolute value)
print(x.dot(3.0))             // 15.0 (multiplication)
print(x.toArray())            // [5.0]
print(Double.dimension)       // 1

// This enables scalar optimization with same API
func optimize<V: VectorSpace>(_ f: (V) -> V.Scalar, ...) -> V

// Works for scalars
let scalarResult: Double = optimize(...)

// And vectors
let vectorResult: VectorN<Double> = optimize(...)
```

---

## 7. Use Cases and Examples

### Example 1: Portfolio Optimization

Calculate portfolio return and risk for a real investment portfolio.

```swift
import BusinessMath

// Portfolio with 3 assets: US Stocks, Bonds, Real Estate
// You've invested: 30% in stocks, 40% in bonds, 30% in real estate
var weights = VectorN([0.3, 0.4, 0.3])

// Verify portfolio is fully invested (weights should sum to 100%)
print("Total allocation: \(weights.sum.percent(0))")  // 100%

// Expected annual returns for each asset
// Stocks: 10%, Bonds: 15%, Real Estate: 12%
let expectedReturns = VectorN([0.10, 0.15, 0.12])

// Calculate expected portfolio return using dot product
// This computes: (0.3 √ó 10%) + (0.4 √ó 15%) + (0.3 √ó 12%)
let portfolioReturn = weights.dot(expectedReturns)
print("\nExpected portfolio return: \(portfolioReturn.percent(1))")  // 12.6%

// If weights don't sum to 100%, normalize them
if abs(weights.sum - 1.0) > 0.001 {
    weights = weights / weights.sum
    print("Weights normalized to: \(weights.toArray())")
}

// Show breakdown by asset
print("\nPortfolio breakdown:")
let assetNames = ["US Stocks", "Bonds", "Real Estate"]
for i in 0..<3 {
    let contribution = weights[i] * expectedReturns[i]
    print("  \(assetNames[i]): \(weights[i].percent(0)) ‚Üí \(contribution.percent(2)) return")
}
```

### Example 2: Finding Optimal Price and Quantity

Use gradient descent to find the price and quantity that minimize costs.

```swift
import BusinessMath

// Cost function: Cost(price, quantity) = price¬≤ + quantity¬≤
// This represents the total cost including pricing complexity and production costs
// Goal: Find the price and quantity that minimize total cost
func totalCost(_ point: Vector2D<Double>) -> Double {
    let price = point.x
    let quantity = point.y
    return price * price + quantity * quantity
}

// The gradient tells us which direction increases cost fastest
// ‚àáCost = [2√óprice, 2√óquantity]
func costGradient(_ point: Vector2D<Double>) -> Vector2D<Double> {
    Vector2D(x: 2 * point.x, y: 2 * point.y)
}

// Start with an initial guess: $10 price, 10 units
var currentPoint = Vector2D(x: 10.0, y: 10.0)
let learningRate = 0.1  // How big a step to take each iteration

print("Starting point: Price=\(currentPoint.x.currency()), Quantity=\(currentPoint.y.number(0))")
print("Starting cost: \(totalCost(currentPoint).currency())\n")

// Iteratively move in the direction that decreases cost
for iteration in 0..<100 {
    // Calculate which direction increases cost
    let grad = costGradient(currentPoint)

    // Move in the opposite direction (decrease cost)
    currentPoint = currentPoint - learningRate * grad

    // Stop if we're close enough to the minimum
    if grad.norm < 0.001 {
        print("Converged after \(iteration + 1) iterations")
        break
    }
}

print("\nOptimal point found:")
print("  Price: \(currentPoint.x.currency())")
print("  Quantity: \(currentPoint.y.number(0))")
print("  Minimum cost: \(totalCost(currentPoint).currency())")
```

### Example 3: Optimal Warehouse Location

Find the best location for a new warehouse that minimizes total delivery distance to customers.

```swift
import BusinessMath

// Customer locations on a grid (in kilometers)
// These represent stores that need daily deliveries
let customers = [
	"Downtown Store": VectorN([2.0, 3.0]),
	"Suburban Mall": VectorN([8.0, 6.0]),
	"Airport Location": VectorN([12.0, 4.0]),
	"Industrial Park": VectorN([5.0, 10.0]),
	"Beach Store": VectorN([15.0, 2.0])
]

// Delivery volumes (parcels per day) - used for weighting
let deliveryVolumes = [
	"Downtown Store": 50.0,
	"Suburban Mall": 30.0,
	"Airport Location": 25.0,
	"Industrial Park": 40.0,
	"Beach Store": 20.0
]

print("=== Finding Optimal Warehouse Location ===\n")

// Method 1: Geometric Median (Centroid) - Simple and Fast
// This minimizes total Euclidean distance for equal weights
var weightedSum = VectorN<Double>.zero
var totalWeight = 0.0

for (name, location) in customers {
	let volume = deliveryVolumes[name]!
	// Multiply location by delivery volume to weight it
	weightedSum = weightedSum + volume * location
	totalWeight += volume
}

// Optimal location is the weighted average (centroid)
let optimalLocation = weightedSum / totalWeight

print("Customer locations:")
for (name, location) in customers.sorted(by: { $0.key < $1.key }) {
	let volume = deliveryVolumes[name]!
	print("  \(name): (\(location[0].number(1)), \(location[1].number(1))) - \(volume.number(0)) parcels/day")
}

print("\nüéØ Optimal warehouse location (weighted centroid):")
print("   Location: (\(optimalLocation[0].number(2)), \(optimalLocation[1].number(2))) km")

// Calculate total weighted distance from optimal location
var totalWeightedDistance = 0.0
print("\n=== Distance from Optimal Location to Each Customer ===")
for (name, location) in customers.sorted(by: { $0.key < $1.key }) {
	let distance = optimalLocation.distance(to: location)
	let volume = deliveryVolumes[name]!
	let weightedDist = distance * volume
	totalWeightedDistance += weightedDist
	print("  \(name): \(distance.number(2)) km √ó \(volume.number(0)) parcels = \(weightedDist.number(1)) km¬∑parcels")
}

print("\nTotal weighted distance: \(totalWeightedDistance.number(1)) km¬∑parcels/day")

// Compare with a suboptimal location (e.g., at origin)
print("\n=== Comparison: Warehouse at Origin (0, 0) ===")
let originLocation = VectorN([0.0, 0.0])
var totalDistanceFromOrigin = 0.0

for (name, location) in customers.sorted(by: { $0.key < $1.key }) {
	let distance = originLocation.distance(to: location)
	let volume = deliveryVolumes[name]!
	let weightedDist = distance * volume
	totalDistanceFromOrigin += weightedDist
	print("  \(name): \(distance.number(2)) km √ó \(volume.number(0)) parcels = \(weightedDist.number(1)) km¬∑parcels")
}

print("\nTotal weighted distance from origin: \(totalDistanceFromOrigin.number(1)) km¬∑parcels/day")

// Show improvement
let improvement = totalDistanceFromOrigin - totalWeightedDistance
let percentImprovement = (improvement / totalDistanceFromOrigin)
print("\nüí∞ Improvement: \(improvement.number(1)) km¬∑parcels/day (\(percentImprovement.percent(1)) reduction)")
print("   Annual savings: \((improvement * 365).number(0)) km¬∑parcels/year")

// Method 2: Iterative Optimization using Gradient Descent
// This finds the true geometric median (Fermat-Weber point)
print("\n=== Iterative Optimization (Gradient Descent) ===")

var currentLocation = optimalLocation  // Start from centroid
let learningRate = 0.1
let maxIterations = 100

for iteration in 0..<maxIterations {
	// Calculate gradient: direction that increases total distance
	var gradient = VectorN<Double>.zero

	for (name, customerLoc) in customers {
		let volume = deliveryVolumes[name]!
		let diff = currentLocation - customerLoc
		let dist = diff.norm

		// Avoid division by zero
		if dist > 0.001 {
			// Gradient of distance function is the unit vector pointing away
			let unitVector = diff / dist
			gradient = gradient + volume * unitVector
		}
	}

	// Move in opposite direction of gradient (toward lower distance)
	currentLocation = currentLocation - learningRate * gradient

	// Check convergence
	if gradient.norm < 0.01 {
		print("Converged after \(iteration + 1) iterations")
		break
	}
}

print("Refined optimal location: (\(currentLocation[0].number(2)), \(currentLocation[1].number(2))) km")

// Calculate final total distance
var finalTotalDistance = 0.0
for (name, location) in customers {
	let distance = currentLocation.distance(to: location)
	let volume = deliveryVolumes[name]!
	finalTotalDistance += distance * volume
}

print("Final total weighted distance: \(finalTotalDistance.number(1)) km¬∑parcels/day")
print("\nThe iteratively optimized location is \((totalWeightedDistance - finalTotalDistance).number(1)) km¬∑parcels/day better than the centroid")
```

### Example 4: Comparing Customer Profiles

Compare customer purchase patterns to find similar customers.

```swift
import BusinessMath

// Customer purchase history across 5 product categories
// Values represent purchase frequency: [Electronics, Clothing, Food, Books, Sports]

// Customer A: Tech enthusiast (lots of electronics, some books)
let customerA = VectorN([5.0, 1.0, 2.0, 4.0, 1.0])

// Customer B: Bookworm (lots of books, some electronics)
let customerB = VectorN([2.0, 1.0, 2.0, 5.0, 1.0])

// Customer C: Athletic (lots of sports, clothing, some food)
let customerC = VectorN([1.0, 4.0, 3.0, 1.0, 5.0])

print("=== Customer Similarity Analysis ===\n")

// Calculate cosine similarity (ranges from -1 to 1, where 1 = identical patterns)
let similarityAB = customerA.cosineSimilarity(with: customerB)
let similarityAC = customerA.cosineSimilarity(with: customerC)
let similarityBC = customerB.cosineSimilarity(with: customerC)

print("Customer A ‚Üî Customer B: \(similarityAB.number(3))")  // 0.888 (somewhat similar)
print("Customer A ‚Üî Customer C: \(similarityAC.number(3))")  // 0.485 (less similar)
print("Customer B ‚Üî Customer C: \(similarityBC.number(3))")  // 0.516 (moderately similar)

// Interpretation
print("\n=== Recommendation ===")
if similarityAB > similarityAC {
	print("Customer A is most similar to Customer B")
	print("‚Üí Recommend Customer A products that Customer B likes (books)")
} else {
	print("Customer A is most similar to Customer C")
	print("‚Üí Recommend Customer A products that Customer C likes (sports, clothing)")
}

// Two types of normalization for different purposes
print("\n=== Type 1: Euclidean Normalization (Unit Length) ===")
let euclideanNormA = customerA.normalized()  // Makes ||v|| = 1
let euclideanNormB = customerB.normalized()

print("Customer A (Euclidean): \(euclideanNormA.toArray().map { $0.number(2) })")
print("  Norm (length): \(euclideanNormA.norm.number(2))")  // Will be 1.0
print("  Sum: \(euclideanNormA.sum.number(2))")  // Will NOT be 1.0!

print("\nCustomer B (Euclidean): \(euclideanNormB.toArray().map { $0.number(2) })")
print("  Norm (length): \(euclideanNormB.norm.number(2))")  // Will be 1.0
print("  Sum: \(euclideanNormB.sum.number(2))")  // Will NOT be 1.0!

print("\n‚ÑπÔ∏è  Euclidean normalization makes the LENGTH = 1, not the sum")
print("   This is correct for cosine similarity (already built into the formula)")

// For proportions/percentages, use sum normalization
print("\n=== Type 2: Sum Normalization (Proportions) ===")
let proportionA = customerA / customerA.sum  // Makes sum = 1
let proportionB = customerB / customerB.sum

print("Customer A (proportions): \(proportionA.toArray().map { $0.number(2) })")
print("  Sum: \(proportionA.sum.number(2))")  // Will be 1.0
print("  Interpretation: Fractions of total purchases")

print("\nCustomer B (proportions): \(proportionB.toArray().map { $0.number(2) })")
print("  Sum: \(proportionB.sum.number(2))")  // Will be 1.0

print("\n‚ÑπÔ∏è  Sum normalization creates percentages/proportions that sum to 100%")
print("   Example: Customer A spends \(proportionA[0].percent()) on electronics")
```

---

## 8. Performance Characteristics

### Vector Type Selection

| Type       | Dimension | Performance | Use When                                          |
|------------|-----------|-------------|---------------------------------------------------|
| `Double`   |         1 | Fastest     | Scalar problems                                   |
| `Vector2D` |         2 | Very fast   | 2D problems (known at compile-time)               |
| `Vector3D` |         3 | Very fast   | 3D problems (known at compile-time)               |
| `VectorN`  |         N | Fast        | Variable/high dimensions, unknown at compile-time |

### Optimization Tips

1. **Use fixed-dimension vectors when possible:**
   ```swift
   // ‚úÖ Better for 2D problems
   let v: Vector2D<Double> = Vector2D(x: 1.0, y: 2.0)

   // ‚ùå Slower for known 2D problems
   let v: VectorN<Double> = VectorN([1.0, 2.0])
   ```

2. **Use squared norms for comparisons:**
   ```swift
   // ‚ùå Slower (unnecessary sqrt)
   if v.norm < 1.0 { ... }

   // ‚úÖ Faster (no sqrt)
   if v.squaredNorm < 1.0 { ... }
   ```

3. **Provide analytical gradients when possible:**
   ```swift
   // ‚ùå Slower (numerical gradient)
   let constraint = MultivariateConstraint.equality { w in w.sum() - 1.0 }

   // ‚úÖ Faster (analytical gradient)
   let constraint = MultivariateConstraint.equality(
       function: { w in w.sum() - 1.0 },
       gradient: { w in VectorN(repeating: 1.0, count: w.dimension) }
   )
   ```

---

## 9. Integration with Optimization

The VectorSpace protocol enables optimization algorithms that work with any vector type. All multivariate optimizers conform to the ``MultivariateOptimizer`` protocol, enabling algorithm swapping and polymorphism.

### Using the Protocol for Algorithm Flexibility

```swift
import BusinessMath

// Using the MultivariateOptimizer protocol enables algorithm flexibility
let optimizer: any MultivariateOptimizer<VectorN<Double>> = MultivariateGradientDescent(
    learningRate: 0.01,
    maxIterations: 1000,
    tolerance: 0.0001
)

// Minimize a function: f(x) = ||x||¬≤ (minimizer is x = [0, 0, 0])
let objective: (VectorN<Double>) -> Double = { x in
    x.squaredNorm  // Sum of squares
}

let result = try optimizer.minimize(objective, from: VectorN([1.0, 2.0, 3.0]))

print("Solution: \(result.solution.toArray())")  // Near [0, 0, 0]
print("Converged: \(result.converged)")
print("Iterations: \(result.iterations)")
```

### Algorithm Swapping at Runtime

The protocol enables comparing multiple algorithms on the same problem:

```swift
import BusinessMath

// Define objective function
let rosenbrock = { (v: VectorN<Double>) -> Double in
    let x = v[0], y = v[1]
    return (1 - x) * (1 - x) + 100 * (y - x*x) * (y - x*x)
}

// Try different algorithms
let algorithms: [any MultivariateOptimizer<VectorN<Double>>] = [
    MultivariateGradientDescent(learningRate: 0.001, maxIterations: 10000),
    MultivariateNewtonRaphson(maxIterations: 100, tolerance: 1e-6),
    AdaptiveOptimizer(maxIterations: 1000)
]

for optimizer in algorithms {
    let result = try optimizer.minimize(rosenbrock, from: VectorN([0.0, 0.0]))
	print("\(type(of: optimizer)): \(result.solution.toArray().map({ $0.number(1) })) \(result.iterations) iterations")
}
```

### Works with Different Vector Types

The protocol works with any VectorSpace-conforming type:

```swift
import BusinessMath

// 2D optimization
let optimizer2D: any MultivariateOptimizer<Vector2D<Double>> = MultivariateGradientDescent(
    learningRate: 0.1,
    maxIterations: 100
)

let result2D = try optimizer2D.minimize(
    { v in v.x * v.x + v.y * v.y },
    from: Vector2D(x: 5.0, y: 5.0)
)

print("2D Solution: (\(result2D.solution.x), \(result2D.solution.y))")  // Near (0, 0)

// 3D optimization
let optimizer3D: any MultivariateOptimizer<Vector3D<Double>> = MultivariateNewtonRaphson(
    maxIterations: 50,
    tolerance: 1e-6
)

let result3D = try optimizer3D.minimize(
    { v in v.x * v.x + v.y * v.y + v.z * v.z },
    from: Vector3D(x: 1.0, y: 2.0, z: 3.0)
)

print("3D Solution: (\(result3D.solution.x), \(result3D.solution.y), \(result3D.solution.z))")
```

**Available Optimizers (all conform to ``MultivariateOptimizer``):**
- ``MultivariateGradientDescent`` - First-order methods (basic, momentum, Adam)
- ``MultivariateNewtonRaphson`` - Second-order methods (Newton, BFGS)
- ``ConstrainedOptimizer`` - Equality constraints only
- ``InequalityOptimizer`` - Mixed equality/inequality constraints
- ``AdaptiveOptimizer`` - Automatic algorithm selection
- ``ParallelOptimizer`` - Parallel multi-start global optimization
- ``StochasticOptimizer`` - Scenario-based optimization under uncertainty
- ``RobustOptimizer`` - Worst-case optimization with parameter uncertainty

---

## 10. Best Practices

### 1. Choose the Right Vector Type

```swift
// ‚úÖ Good: Known 2D problem
let point: Vector2D<Double> = Vector2D(x: 1.0, y: 2.0)

// ‚úÖ Good: Variable dimension
let weights: VectorN<Double> = VectorN(portfolioWeights)

// ‚ùå Bad: Using VectorN for fixed 2D
let point: VectorN<Double> = VectorN([1.0, 2.0])  // Slower!
```

### 2. Check Dimensions

```swift
// ‚ùå Bad: Assuming dimensions match
let result = v.dot(w)

// ‚úÖ Good: Verify dimensions
guard v.dimension == w.dimension else {
    throw OptimizationError.dimensionMismatch
}
let result = v.dot(w)
```

### 3. Use isFinite for Validation

```swift
// ‚úÖ Good: Validate numeric stability
let gradient = computeGradient(at: x)
guard gradient.isFinite else {
    throw OptimizationError.numericalInstability
}
```

### 4. Leverage Functional Operations

```swift
// ‚ùå Verbose: Manual loop
var result = VectorN<Double>([])
for i in 0..<v.dimension {
    result = result.appending(v[i] * v[i])
}

// ‚úÖ Concise: Functional style
let result = v.map { $0 * $0 }
```

---

## Summary

This module's VectorSpace foundation provides:

‚úÖ **Generic vector operations** via VectorSpace protocol
‚úÖ **Multiple vector types** (Vector2D, Vector3D, VectorN) for different use cases
‚úÖ **Rich operations** (norms, distances, projections, etc.)
‚úÖ **Type-safe constraints** with MultivariateConstraint
‚úÖ **Functional programming** support (map, filter, reduce, etc.)

This foundation enables:
- Generic optimization algorithms via the ``MultivariateOptimizer`` protocol
- Constrained optimization with ``ConstrainedOptimizer`` and ``InequalityOptimizer``
- Business optimization modules for portfolio allocation and resource planning

## Next Steps

- Learn <doc:5.1.5-MultivariateOptimizerGuide> for algorithm flexibility and polymorphism
- Explore <doc:5.5-MultivariateOptimization> for complete optimization tutorials
- Add <doc:5.6-ConstrainedOptimization> to handle complex constraints
- Apply <doc:5.7-BusinessOptimization> to real-world business problems

## See Also

- ``VectorSpace``
- ``VectorN``
- ``Vector2D``
- ``Vector3D``
- ``MultivariateOptimizer``
- ``MultivariateGradientDescent``
- ``MultivariateNewtonRaphson``
- ``ConstrainedOptimizer``
- ``InequalityOptimizer``
- ``MultivariateConstraint``
