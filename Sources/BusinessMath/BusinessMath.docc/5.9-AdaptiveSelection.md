# Adaptive Algorithm Selection

Automatically analyze your problem and select the best algorithm, adapting parameters for optimal performance.

---

## Table of Contents

1. [Overview](#overview)
2. [Quick Start](#quick-start)
3. [How It Works](#how-it-works)
4. [Algorithm Selection Rules](#algorithm-selection-rules)
5. [User Preferences](#user-preferences)
6. [Real-World Examples](#real-world-examples)
7. [Understanding Results](#understanding-results)
8. [Troubleshooting](#troubleshooting)
9. [Best Practices](#best-practices)

---

## Overview

The Adaptive Optimizer eliminates the complexity of choosing between different optimization algorithms. It automatically analyzes your problem and selects the best algorithm, adapting parameters for optimal performance.

### What Problems Does It Solve?

**Before Adaptive Selection:**
- Users must understand algorithm trade-offs (Gradient Descent vs Newton-Raphson vs BFGS)
- Guessing learning rates and parameters leads to poor convergence
- Wrong algorithm choice wastes time or produces bad results
- Trial-and-error is frustrating and inefficient

**After Adaptive Selection:**
- Just call `optimize()` - automatic algorithm selection
- Parameters automatically adapted to problem size
- Transparent reasoning explains choices
- Optimal performance without expertise

### Key Features

- **Automatic Algorithm Selection**: Chooses from Gradient Descent, Newton-Raphson, Constrained Optimizer, Inequality Optimizer
- **Adaptive Parameters**: Learning rates adjust to problem size (0.001-0.05 range)
- **Smart Rules**: Constraints → Problem size → User preferences
- **Transparent Reasoning**: Every result explains the choice
- **100% Test Coverage**: Rock-solid reliability

---

## Quick Start

### Simplest Possible Use (3 lines)

```swift
import BusinessMath

let optimizer = AdaptiveOptimizer<VectorN<Double>>()

let result = try optimizer.optimize(
	objective: { v in
		let x = v[0], y = v[1]
		return (x - 1) * (x - 1) + (y - 2) * (y - 2) },
	initialGuess: VectorN([0.0, 0.0])
)

print("Solution: [\(result.solution.toArray().map({ $0.number(0) }).joined(separator: ", "))]")           // [1.0, 2.0]
print("Algorithm: \(result.algorithmUsed)")     // "Newton-Raphson"
print("Why: \(result.selectionReason)")         // "Small problem (2 variables)"
```

That's it! No algorithm choice, no parameter tuning, no guesswork.

---

## How It Works

### The Decision Process

When you call `optimize()`, the Adaptive Optimizer:

1. **Analyzes Your Problem**
   - How many variables? (problem size)
   - Any constraints? (equality, inequality, or unconstrained)
   - Gradient provided? (analytical vs numerical)
   - User preferences? (speed vs accuracy)

2. **Selects Best Algorithm**
   - Routes to optimal algorithm based on rules
   - Falls through decision tree systematically
   - Always makes a choice (no ambiguity)

3. **Adapts Parameters**
   - Learning rates based on problem size
   - Tolerance settings from configuration
   - Maximum iterations from settings

4. **Returns Transparent Results**
   - Which algorithm was used
   - Why it was chosen
   - Solution quality metrics

### Under the Hood

```swift
// You write this:
let result = try optimizer.optimize(objective: f, initialGuess: x0)

// Adaptive Optimizer does this:
let problemSize = x0.toArray().count          // Analyze
let hasConstraints = !constraints.isEmpty
let hasInequalities = constraints.contains { !$0.isEquality }

// Select algorithm
if hasInequalities {
    use InequalityOptimizer  // Best for inequality constraints
} else if hasConstraints {
    use ConstrainedOptimizer  // Best for equality constraints
} else if problemSize > 100 {
    use GradientDescent  // Best for large problems
} else if problemSize <= 5 {
    use NewtonRaphson  // Best for small problems
} else {
    use GradientDescent  // Balanced default
}

// Adapt learning rate
if problemSize > 100 {
    learningRate = 0.01  // Larger for big problems
} else {
    learningRate = 0.001  // Smaller for stability
}
```

---

## Algorithm Selection Rules

### Rule 1: Inequality Constraints → Inequality Optimizer

**When:** Problem has any inequality constraint (≤ or ≥)

**Why:** Specialized penalty-barrier method handles inequalities efficiently

**Example:**
```swift
let optimizer = AdaptiveOptimizer<VectorN<Double>>()

let result = try optimizer.optimize(
	objective: { v in
		let x = v[0], y = v[1]
		return x * x + y * y
	},
	initialGuess: VectorN([1.0, 1.0]),
	constraints: [
		.inequality(function: { x in -x[0] }, gradient: nil)  // x[0] >= 0
	]
)

print("[\(result.solution.toArray().map({ $0.number(2) }).joined(separator: ", "))]")
print(result.algorithmUsed)  // "Inequality Optimizer"
print(result.selectionReason)  // "Problem has inequality constraints"
```

### Rule 2: Equality Constraints → Constrained Optimizer

**When:** Problem has only equality constraints (=)

**Why:** Augmented Lagrangian method is optimal for equalities

**Example:**
```swift
let optimizer = AdaptiveOptimizer<VectorN<Double>>()

let result = try optimizer.optimize(
	objective: { x in x[0] * x[0] + x[1] * x[1] },
	initialGuess: VectorN([1.0, 1.0]),
	constraints: [
		.equality(function: { x in x[0] + x[1] - 1.0 }, gradient: nil)
	]
)

print("[\(result.solution.toArray().map({ $0.number(2) }).joined(separator: ", "))]")
print(result.algorithmUsed)  // "Constrained Optimizer"
print(result.selectionReason)  // "Problem has equality constraints"
```

### Rule 3: Large Problems (>100 variables) → Gradient Descent

**When:** Problem has more than 100 variables

**Why:** Memory-efficient, scales well, fast convergence for large N

**Example:**
```swift
let optimizer = AdaptiveOptimizer<VectorN<Double>>()

let dimension = 150
let initial = VectorN(Array(repeating: 0.5, count: dimension))

let result = try optimizer.optimize(
	objective: { x in x.toArray().reduce(0.0) { $0 + $1 * $1 } },
	initialGuess: initial
)

print("[\(result.solution.toArray().map({ $0.number(2) }).joined(separator: ", "))]")
print(result.algorithmUsed)  // "Gradient Descent"
print(result.selectionReason)  // "Large problem (150 variables)"
```

**Learning Rate:** 0.01 (10x larger than small problems for faster convergence)

### Rule 4: Accuracy Preference → Newton-Raphson

**When:** User sets `preferAccuracy: true` and problem < 10 variables

**Why:** Newton-Raphson has quadratic convergence for ultimate accuracy

**Example:**
```swift
let optimizer = AdaptiveOptimizer<VectorN<Double>>()

let result = try optimizer.optimize(
	objective: { v in
		let x = v[0], y = v[1]
		return x * x + y * y },
	initialGuess: VectorN([1.0, 1.0])
)

print("[\(result.solution.toArray().map({ $0.number(2) }).joined(separator: ", "))]")
print(result.algorithmUsed)  // "Newton-Raphson"
print(result.selectionReason)  // "Accuracy preference with small problem"
```

### Rule 5: Very Small Problems (≤5 variables) → Newton-Raphson

**When:** Problem has 5 or fewer variables (unless `preferSpeed: true`)

**Why:** Newton-Raphson's O(n³) Hessian cost is negligible for n≤5, convergence vastly superior

**Example:**
```swift
let optimizer = AdaptiveOptimizer<VectorN<Double>>()

let rosenbrock: (VectorN<Double>) -> Double = { v in
	let x = v[0], y = v[1]
	return (1 - x) * (1 - x) + 100 * (y - x*x) * (y - x*x)
}

let result = try optimizer.optimize(
	objective: rosenbrock,
	initialGuess: VectorN([0.0, 0.0])
)

print("[\(result.solution.toArray().map({ $0.number(2) }).joined(separator: ", "))]")
print(result.algorithmUsed)  // "Newton-Raphson"
print(result.selectionReason)  // "Small problem (2 variables)"
```

**Note:** Rosenbrock is notoriously difficult - Newton-Raphson handles it perfectly!

### Rule 6: Default → Gradient Descent

**When:** Medium unconstrained problem (6-100 variables)

**Why:** Best balance of speed, memory, and convergence

**Example:**
```swift
let optimizer = AdaptiveOptimizer<VectorN<Double>>()

let result = try optimizer.optimize(
	objective: { x in
		// 10-dimensional quadratic
		x.toArray().reduce(0.0) { $0 + $1 * $1 }
	},
	initialGuess: VectorN(Array(repeating: 1.0, count: 10))
)

print("[\(result.solution.toArray().map({ $0.number(2) }).joined(separator: ", "))]")
print(result.algorithmUsed)  // "Gradient Descent"
print(result.selectionReason)  // "Unconstrained problem - using gradient descent"
```

**Learning Rate:** 0.001 (conservative for stability)

---

## User Preferences

### Speed vs Accuracy Trade-off

You can guide the algorithm selection with preferences:

#### Prefer Speed

```swift
let optimizer = AdaptiveOptimizer<VectorN<Double>>(preferSpeed: true)

let result = try optimizer.optimize(
	objective: { v in
		let x = v[0], y = v[1]
		return x * x + y * y
	},
	gradient: { v in
		let x = v[0], y = v[1]
		return VectorN([2 * x, 2 * y])
	},  // Analytical gradient
	initialGuess: VectorN([1.0, 1.0])
)

print("[\(result.solution.toArray().map({ $0.number(2) }).joined(separator: ", "))]")
print(result.algorithmUsed)  // "Gradient Descent"
// Faster algorithms prioritized, even for small problems
print(result.selectionReason)  // "Unconstrained problem - using gradient descent"
```

**When to use:**
- Need quick approximate solutions
- Running many optimizations
- Real-time applications
- Interactive tools

#### Prefer Accuracy

```swift
let optimizer = AdaptiveOptimizer<VectorN<Double>>(preferAccuracy: true)

let result = try optimizer.optimize(
	objective: { v in
		let x = v[0], y = v[1]
		return x * x + y * y
	},
	initialGuess: VectorN([1.0, 1.0])
)


print("[\(result.solution.toArray().map({ $0.number(2) }).joined(separator: ", "))]")
print(result.algorithmUsed)  // "Newton-Raphson"
							 // Most accurate algorithms chosen
print(result.selectionReason)  // "Accuracy preference with small problem - using full Newton-Raphson"
```

**When to use:**
- Need precise solutions
- Financial calculations
- Scientific computing
- Final production runs

### Custom Tolerances

```swift
let optimizer = AdaptiveOptimizer<VectorN<Double>>(
    maxIterations: 5000,   // More iterations (default: 1000
	tolerance: 1e-8      // Very tight (default: 1e-6)
   )
)

let result = try optimizer.optimize(
    objective: complexFunction,
    initialGuess: initialGuess
)

print(result.objectiveValue)  // Very precise result
```

---

## Real-World Examples

### Example 1: Portfolio Optimization

```swift
import BusinessMath

// Minimize portfolio variance subject to constraints
let optimizer = AdaptiveOptimizer<VectorN<Double>>()

// Covariance matrix (3 assets)
let covariance = [
	[0.04, 0.01, 0.02],
	[0.01, 0.09, 0.015],
	[0.02, 0.015, 0.16]
]

let result = try optimizer.optimize(
	objective: { weights in
		// Portfolio variance: w'Σw
		let w = weights.toArray()
		var variance = 0.0
		for i in 0..<3 {
			for j in 0..<3 {
				variance += w[i] * covariance[i][j] * w[j]
			}
		}
		return variance
	},
	initialGuess: VectorN([1.0/3, 1.0/3, 1.0/3]),  // Equal weights
	constraints: [
		.equality(function: { x in x.toArray().reduce(0, +) - 1.0 }, gradient: nil),
		.inequality(function: { x in -x[0] }, gradient: nil),
		.inequality(function: { x in -x[1] }, gradient: nil),
		.inequality(function: { x in -x[2] }, gradient: nil)
	]
)

print("Algorithm: \(result.algorithmUsed)")  // "Inequality Optimizer"
print("[\(result.solution.toArray().map({ $0.number(2) }).joined(separator: ", "))]")
print("Variance: \(result.objectiveValue.percent(3))")

// Check constraints
let sum = result.solution.toArray().reduce(0, +)
print("Sum of weights: \(sum.number(2))")  // Should be 1.0
```

**What Happened:**
- Adaptive Optimizer detected inequality constraints (non-negativity)
- Selected Inequality Optimizer automatically
- Converged to optimal minimum-variance portfolio
- All constraints satisfied

### Example 2: Production Planning

```swift
// Maximize profit from two products with capacity constraints
let optimizer = AdaptiveOptimizer<VectorN<Double>>()

let result = try optimizer.optimize(
	objective: { x in
		let product1 = x[0], product2 = x[1]
		let revenue = 50 * product1 + 40 * product2
		let cost = 20 * product1 + 15 * product2
		return -(revenue - cost)  // Negative for minimization
	},
	initialGuess: VectorN([10.0, 10.0]),
	constraints: [
		// Labor constraint: 2*p1 + 3*p2 <= 100
		.inequality(function: { x in 2*x[0] + 3*x[1] - 100 }, gradient: nil),
		// Material constraint: 4*p1 + 2*p2 <= 120
		.inequality(function: { x in 4*x[0] + 2*x[1] - 120 }, gradient: nil),
		// Non-negativity
		.inequality(function: { x in -x[0] }, gradient: nil),
		.inequality(function: { x in -x[1] }, gradient: nil)
	]
)

let product1 = result.solution[0]
let product2 = result.solution[1]
let profit = -result.objectiveValue

print("Algorithm: \(result.algorithmUsed)")  // "Inequality Optimizer"
print("Product 1: \(product1.number(0)) units")
print("Product 2: \(product2.number(0)) units")
print("Maximum profit: \(profit.currency())")
```

### Example 3: Parameter Estimation (Small Problem)

```swift
// Calibrate model parameters to fit observed data
let observedData = [3.7, 4.6, 5.9, 7.6, 10.0]
let timePoints = [1.0, 2.0, 3.0, 4.0, 5.0]

func model(params: VectorN<Double>, t: Double) -> Double {
	let a = params[0], b = params[1], c = params[2]
	return a * exp(b * t) + c
}

let optimizer = AdaptiveOptimizer<VectorN<Double>>(preferAccuracy: true)

let result = try optimizer.optimize(
	objective: { params in
		// Sum of squared errors
		var sse = 0.0
		for (t, observed) in zip(timePoints, observedData) {
			let predicted = model(params: params, t: t)
			sse += (observed - predicted) * (observed - predicted)
		}
		return sse
	},
	initialGuess: VectorN([1.0, 0.5, 0.0])
)

print("Algorithm: \(result.algorithmUsed)")  // "Newton-Raphson"
print("Reason: \(result.selectionReason)")  // "Accuracy preference with small problem"
print("Parameters: [\(result.solution.toArray().map({ $0.number(2) }).joined(separator: ", "))]")
print("Fit error: \(result.objectiveValue.number(2))")
```

**Why Newton-Raphson:**
- Small problem (3 parameters)
- Accuracy preference set
- Quadratic convergence for precise parameter estimates

### Example 4: Large-Scale Optimization

```swift
// 200-dimensional optimization (e.g., neural network weights)
let dimension = 200
let optimizer = AdaptiveOptimizer<VectorN<Double>>()

let result = try optimizer.optimize(
    objective: { x in
        // Complex high-dimensional function
        let values = x.toArray()
        var sum = 0.0
        for i in 0..<dimension {
            sum += values[i] * values[i]
            if i > 0 {
                sum += 0.1 * values[i-1] * values[i]
            }
        }
        return sum
    },
    initialGuess: VectorN(Array(repeating: 0.5, count: dimension))
)

print("Algorithm: \(result.algorithmUsed)")  // "Gradient Descent"
print("Reason: \(result.selectionReason)")  // "Large problem (200 variables)"
print("Converged in \(result.iterations) iterations")
```

**Why Gradient Descent:**
- Large problem (200 variables)
- Memory efficient (no Hessian matrix)
- Adaptive learning rate (0.01 for faster convergence)
- Scales well to high dimensions

---

## Understanding Results

### Result Structure

```swift
public struct Result {
    public let solution: V                    // Optimal solution vector
    public let objectiveValue: V.Scalar       // Objective at solution
    public let algorithmUsed: String          // Which algorithm was selected
    public let selectionReason: String        // Why it was chosen
    public let iterations: Int                // Number of iterations
    public let converged: Bool                // Whether optimization converged
    public let constraintViolation: V.Scalar? // Constraint violation (if any)
}
```

### Interpreting Results

#### Successful Convergence

```swift
let result = try optimizer.optimize(objective: f, initialGuess: x0)

if result.converged {
    print("✅ Optimization succeeded!")
    print("Solution: \(result.solution)")
    print("Objective: \(result.objectiveValue)")
    print("Used \(result.algorithmUsed) (\(result.iterations) iterations)")
    print("Reason: \(result.selectionReason)")

    if let violation = result.constraintViolation {
        print("Constraint violation: \(violation)")  // Should be near zero
    }
}
```

#### Non-Convergence

```swift
if !result.converged {
    print("⚠️ Did not converge after \(result.iterations) iterations")
    print("Algorithm tried: \(result.algorithmUsed)")
    print("Best objective found: \(result.objectiveValue)")

    // Strategies to fix:
    // 1. Increase maxIterations
    // 2. Relax tolerance
    // 3. Try different initial guess
    // 4. Check problem formulation
}
```

---

## Troubleshooting

### Problem: Optimization Not Converging

**Symptoms:**
- `result.converged == false`
- High iteration count
- Objective not improving

**Solutions:**

1. **Increase Iterations**
   ```swift
   let optimizer = AdaptiveOptimizer<VectorN<Double>>(maxIterations: 5000)
   ```

2. **Relax Tolerance**
   ```swift
   let optimizer = AdaptiveOptimizer<VectorN<Double>>(tolerance: 1e-4)
   ```

3. **Better Initial Guess**
   ```swift
   // Instead of zeros:
   let initial = VectorN([0.0, 0.0])  // Bad

   // Use domain knowledge:
   let initial = VectorN([1.0, 2.0])  // Better
   ```

4. **Override Algorithm (if needed)**
   ```swift
   // Force accuracy for difficult problems
   let optimizer = AdaptiveOptimizer<VectorN<Double>>(preferAccuracy: true)
   ```

### Problem: Wrong Algorithm Selected

**Symptoms:**
- Algorithm choice doesn't match expectations
- Poor performance on your specific problem

**Solution:**

Use preferences to guide selection:
```swift
// Force faster algorithms
let fastOptimizer = AdaptiveOptimizer<VectorN<Double>>(preferSpeed: true)

// Force more accurate algorithms
let accurateOptimizer = AdaptiveOptimizer<VectorN<Double>>(preferAccuracy: true)
```

### Problem: Slow Convergence on Small Problems

**Likely Cause:** Gradient Descent selected instead of Newton-Raphson

**Solution:**

```swift
// For problems with ≤5 variables, Newton-Raphson is auto-selected
// For 6-10 variables, force it with preferAccuracy:
let optimizer = AdaptiveOptimizer<VectorN<Double>>(preferAccuracy: true)
```

### Problem: Out of Memory on Large Problems

**Symptoms:**
- Crash or slowdown on large problems
- Memory pressure warnings

**Why:** Newton-Raphson uses O(n²) memory for Hessian

**Solution:**

Adaptive Optimizer automatically uses Gradient Descent for n>100, which is O(n) memory. If you're hitting memory issues with smaller problems:

```swift
let optimizer = AdaptiveOptimizer<VectorN<Double>>(preferSpeed: true)
// Forces Gradient Descent even for small problems
```

---

## Best Practices

### 1. Trust the Automatic Selection

```swift
// ✅ GOOD: Let Adaptive Optimizer decide
let optimizer = AdaptiveOptimizer<VectorN<Double>>()
let result = try optimizer.optimize(objective: f, initialGuess: x0)

// ❌ BAD: Don't manually choose unless you have specific needs
let manualOptimizer = MultivariateGradientDescent<VectorN<Double>>(...)
```

**Why:** The automatic selection is based on optimization theory and empirical testing. It chooses the algorithm most likely to succeed.

### 2. Check the Reasoning

```swift
let result = try optimizer.optimize(objective: f, initialGuess: x0)

print("Used: \(result.algorithmUsed)")
print("Why: \(result.selectionReason)")

// Understand WHY this choice was made
// Learn patterns for your problem domain
```

### 3. Use Preferences for Domain Knowledge

```swift
// Financial calculations: prefer accuracy
let financialOptimizer = AdaptiveOptimizer<VectorN<Double>>(
    preferAccuracy: true,
    tolerance: 1e-8
)

// Interactive tools: prefer speed
let interactiveOptimizer = AdaptiveOptimizer<VectorN<Double>>(
    preferSpeed: true,
    tolerance: 1e-4
)

// Machine learning: balanced default
let mlOptimizer = AdaptiveOptimizer<VectorN<Double>>()
```

### 4. Provide Good Initial Guesses

```swift
// ❌ BAD: Zero initialization for constrained problems
let bad = VectorN([0.0, 0.0, 0.0])

// ✅ GOOD: Feasible point respecting constraints
let good = VectorN([1.0/3, 1.0/3, 1.0/3])  // For budget = 1.0
```

### 5. Validate Results

```swift
let result = try optimizer.optimize(
    objective: objective,
    initialGuess: initial,
    constraints: constraints
)

// Always check:
assert(result.converged, "Optimization did not converge")

// Verify constraints for constrained problems
if let violation = result.constraintViolation {
    assert(violation < 1e-6, "Constraints violated")
}

// Verify solution makes business sense
assert(result.solution.toArray().allSatisfy { $0 >= 0 }, "Negative values unexpected")
```

### 6. Use Problem Analysis for Debugging

```swift
let optimizer = AdaptiveOptimizer<VectorN<Double>>()

// Analyze before optimizing
let analysis = optimizer.analyzeProblem(
    initialGuess: initial,
    constraints: constraints,
    hasGradient: true
)

print("Problem size: \(analysis.size)")
print("Has constraints: \(analysis.hasConstraints)")
print("Recommended: \(analysis.recommendedAlgorithm)")
print("Reason: \(analysis.reason)")

// Now optimize with knowledge of what to expect
let result = try optimizer.optimize(...)
```

### 7. Benchmark for Production Systems

```swift
// Development: Use defaults
let devOptimizer = AdaptiveOptimizer<VectorN<Double>>()

// Production: Tune based on benchmarks
let prodOptimizer = AdaptiveOptimizer<VectorN<Double>>(
    tolerance: 1e-5,        // Validated tolerance
    maxIterations: 2000     // Based on typical problem size
)

// See Performance Benchmarking tutorial for measurement tools
```

---

## Summary

### Key Takeaways

1. **Zero Configuration**: Works out of the box with no algorithm knowledge required
2. **Smart Defaults**: Automatic selection based on problem characteristics
3. **Transparent**: Always explains why an algorithm was chosen
4. **Adaptive**: Parameters tune to problem size automatically
5. **Overrideable**: Preferences available when you need control
6. **Production Ready**: 100% test coverage, all edge cases handled

### When to Use Adaptive Optimizer

**✅ USE for:**
- Any optimization problem (unconstrained or constrained)
- When you're unsure which algorithm is best
- When you want optimal performance without tuning
- Production systems where reliability matters
- Exploratory analysis of new problems

**⚠️ DON'T USE when:**
- You need absolute control over algorithm internals
- You're implementing research algorithms
- You have exotic constraints not supported
- Performance benchmarking (use PerformanceBenchmark tool instead)

### Next Steps

1. **Try it**: Replace manual optimizer choices with AdaptiveOptimizer
2. **Observe**: Check `algorithmUsed` and `selectionReason` to learn patterns
3. **Tune**: Use preferences (speed/accuracy) if defaults don't fit
4. **Benchmark**: Use PerformanceBenchmark (see separate tutorial) to validate

---

## Additional Resources

- **API Reference**: [AdaptiveOptimizer.swift](../Sources/BusinessMath/Optimization/AdaptiveOptimizer.swift)
- **Tests**: [AdaptiveOptimizerTests.swift](../Tests/BusinessMathTests/Performance%20Tests/AdaptiveOptimizerTests.swift)
