# Phase 2: VectorSpace Foundation - Tutorial

**Status:** ✅ Complete
**Phase Goal:** Generic vector operations foundation for multivariate optimization

---

## Overview

Phase 2 introduced the `VectorSpace` protocol and related infrastructure, enabling BusinessMath to work with vectors of any dimension in a type-safe, generic way.

**Key Components:**
1. **VectorSpace Protocol** - Generic interface for vector operations
2. **Vector Implementations** - Vector2D, Vector3D, VectorN
3. **MultivariateConstraint** - Type-safe constraint specification
4. **Extended Operations** - Distance metrics, projections, transformations

This foundation powers all multivariate optimization in Phases 3-5.

---

## 1. The VectorSpace Protocol

### What is a Vector Space?

A **vector space** is a mathematical structure supporting:
- **Vector addition**: v + w
- **Scalar multiplication**: α · v
- **Zero element**: 0
- **Norms and distances**: ‖v‖, ‖v - w‖

### Protocol Definition

```swift
public protocol VectorSpace: AdditiveArithmetic, Hashable, Codable, Sendable {
    associatedtype Scalar: Real & Sendable & Codable

    // Required operations
    static var zero: Self { get }
    static func + (lhs: Self, rhs: Self) -> Self
    static func * (lhs: Scalar, rhs: Self) -> Self
    static prefix func - (vector: Self) -> Self

    // Norm and distance
    var norm: Scalar { get }
    func dot(_ other: Self) -> Scalar

    // Conversion
    static func fromArray(_ array: [Scalar]) -> Self?
    func toArray() -> [Scalar]

    // Dimension
    static var dimension: Int { get }
    var isFinite: Bool { get }
}
```

### Why VectorSpace?

**Problem:** Different vector types (2D, 3D, N-dimensional) had duplicate code

**Solution:** Single protocol + generic algorithms work for all vectors

```swift
// ❌ Before Phase 2: Duplicate implementations
func optimize2D(_ f: (Vector2D) -> Double, ...) -> Vector2D
func optimize3D(_ f: (Vector3D) -> Double, ...) -> Vector3D
func optimizeND(_ f: (VectorN) -> Double, ...) -> VectorN

// ✅ After Phase 2: One generic implementation
func optimize<V: VectorSpace>(_ f: (V) -> V.Scalar, ...) -> V
```

---

## 2. Vector Implementations

### Vector2D - Fixed 2D Vectors

**Use Cases:**
- 2D coordinate systems
- Complex numbers (x = real, y = imaginary)
- Two-variable optimization
- Graphics and geometry

**Performance:** Fastest (compile-time optimization, no array overhead)

```swift
import BusinessMath

// Create a 2D vector
let v = Vector2D<Double>(x: 3.0, y: 4.0)

// Basic operations
let w = Vector2D(x: 1.0, y: 2.0)
let sum = v + w                    // Vector2D(x: 4.0, y: 6.0)
let scaled = 2.0 * v               // Vector2D(x: 6.0, y: 8.0)

// Norm and distance
print(v.norm)                      // 5.0 (√(3² + 4²))
print(v.distance(to: w))           // 2.828...

// Dot product
print(v.dot(w))                    // 11.0 (3*1 + 4*2)

// 2D-specific operations
print(v.cross(w))                  // 2.0 (pseudo-cross product)
print(v.angle)                     // 0.927... radians
let rotated = v.rotated(by: .pi/2) // Vector2D(x: -4.0, y: 3.0)
```

### Vector3D - Fixed 3D Vectors

**Use Cases:**
- 3D coordinate systems
- RGB color spaces
- Three-variable optimization
- Cross product calculations

**Performance:** Very fast (compile-time optimization)

```swift
import BusinessMath

// Create a 3D vector
let v = Vector3D<Double>(x: 1.0, y: 2.0, z: 3.0)
let w = Vector3D<Double>(x: 4.0, y: 5.0, z: 6.0)

// Basic operations
let sum = v + w               // Vector3D(x: 5.0, y: 7.0, z: 9.0)
let scaled = 2.0 * v          // Vector3D(x: 2.0, y: 4.0, z: 6.0)

// Norm and dot product
print(v.norm)                 // 3.742... (√(1² + 2² + 3²))
print(v.dot(w))               // 32.0 (1*4 + 2*5 + 3*6)

// 3D-specific operations
let cross = v.cross(w)        // Vector3D perpendicular to both
print(cross)                  // Vector3D(x: -3.0, y: 6.0, z: -3.0)

// Verify perpendicularity
print(v.dot(cross))           // ~0.0
print(w.dot(cross))           // ~0.0

// Triple products
let tripleScalar = v.tripleProduct(w, v)  // Signed volume
let tripleVector = v.vectorTripleProduct(w, v)
```

### VectorN - Variable N-Dimensional Vectors

**Use Cases:**
- High-dimensional optimization
- Machine learning feature vectors
- Portfolio weights (N assets)
- Any variable or large dimension problem

**Performance:** Flexible but has array bounds checking overhead

```swift
import BusinessMath

// Create an N-dimensional vector
let v = VectorN<Double>([1.0, 2.0, 3.0, 4.0, 5.0])

// Basic operations
let w = VectorN([5.0, 4.0, 3.0, 2.0, 1.0])
let sum = v + w               // VectorN([6, 6, 6, 6, 6])
let scaled = 2.0 * v          // VectorN([2, 4, 6, 8, 10])

// Norm and dot product
print(v.norm)                 // 7.416... (√55)
print(v.dot(w))               // 35.0

// Element access
print(v[0])                   // 1.0
print(v[2])                   // 3.0

// Dimension
print(v.dimension)            // 5
print(v.count)                // 5

// Statistical operations
print(v.sum)                  // 15.0
print(v.mean)                 // 3.0
print(v.standardDeviation())  // 1.581...
print(v.min)                  // 1.0
print(v.max)                  // 5.0

// Element-wise operations
let product = v.hadamardProduct(with: w)  // [5, 8, 9, 8, 5]
let quotient = v.elementwiseDivide(by: w) // [0.2, 0.5, 1.0, 2.0, 5.0]
```

---

## 3. Common Operations

All vector types share these operations through the `VectorSpace` protocol:

### Arithmetic Operations

```swift
let v = VectorN([1.0, 2.0, 3.0])
let w = VectorN([4.0, 5.0, 6.0])

// Addition and subtraction
let sum = v + w               // [5, 7, 9]
let diff = v - w              // [-3, -3, -3]

// Scalar multiplication
let scaled = 3.0 * v          // [3, 6, 9]
let divided = v / 2.0         // [0.5, 1.0, 1.5]

// Negation
let negated = -v              // [-1, -2, -3]
```

### Norms and Distances

```swift
let v = VectorN([3.0, 4.0])

// Norms
print(v.norm)                 // 5.0 (Euclidean: √(3² + 4²))
print(v.squaredNorm)          // 25.0 (faster for comparisons)

// Distance metrics
let w = VectorN([0.0, 0.0])
print(v.distance(to: w))      // 5.0 (Euclidean distance)
print(v.manhattanDistance(to: w))    // 7.0 (|3| + |4|)
print(v.chebyshevDistance(to: w))    // 4.0 (max(|3|, |4|))
```

### Dot Products and Angles

```swift
let v = VectorN([1.0, 0.0, 0.0])
let w = VectorN([0.0, 1.0, 0.0])

// Dot product
print(v.dot(w))               // 0.0 (perpendicular)

// Cosine similarity
print(v.cosineSimilarity(with: w))  // 0.0 (orthogonal)

// Angle between vectors
let angle = v.angle(with: w)  // π/2 radians (90 degrees)
```

### Projections

```swift
let v = VectorN([3.0, 4.0])
let w = VectorN([1.0, 0.0])

// Project v onto w
let projection = v.projection(onto: w)  // [3.0, 0.0]

// Rejection (component perpendicular to w)
let rejection = v.rejection(from: w)    // [0.0, 4.0]

// Verify: v = projection + rejection
assert(v == projection + rejection)
```

### Normalization

```swift
let v = VectorN([3.0, 4.0])

// Normalize to unit length
let unit = v.normalized()     // [0.6, 0.8]
print(unit.norm)              // 1.0

// Verify direction preserved
print(v.cosineSimilarity(with: unit))  // 1.0 (same direction)
```

---

## 4. VectorN-Specific Operations

### Construction

```swift
// From array
let v1 = VectorN([1.0, 2.0, 3.0])

// Repeating value
let v2 = VectorN(repeating: 5.0, count: 10)

// Zero vector
let v3 = VectorN<Double>.zero

// Ones vector
let v4 = VectorN.ones(dimension: 5)

// Basis vector (one component = 1, rest = 0)
let e2 = VectorN<Double>.basisVector(dimension: 5, index: 2)
// [0, 0, 1, 0, 0]

// Linear space (evenly spaced)
let v5 = VectorN.linearSpace(from: 0.0, to: 10.0, count: 11)
// [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

// Log space (logarithmically spaced)
let v6 = VectorN.logSpace(from: 1.0, to: 100.0, count: 3)
// [1, 10, 100]
```

### Manipulation

```swift
var v = VectorN([1.0, 2.0, 3.0])

// Access elements
print(v[0])                   // 1.0
v[1] = 5.0                    // v = [1, 5, 3]

// Append/remove
let v2 = v.appending(4.0)     // [1, 5, 3, 4]
let v3 = v2.removingLast()    // [1, 5, 3]

// Concatenate
let w = VectorN([6.0, 7.0])
let combined = v.concatenated(with: w)  // [1, 5, 3, 6, 7]

// Slice
let slice = combined.slice(1..<4)  // [5, 3, 6]
```

### Functional Operations

```swift
let v = VectorN([1.0, 2.0, 3.0])

// Map (element-wise transform)
let squared = v.map { $0 * $0 }  // [1, 4, 9]

// Filter
let positive = v.filter { $0 > 0 }  // [1, 2, 3]

// Reduce
let sum = v.reduce(0.0, +)  // 6.0

// Zip with another vector
let w = VectorN([4.0, 5.0, 6.0])
let product = v.zipWith(w, *)  // [4, 10, 18]
```

### Orthogonality and Parallelism

```swift
let v = VectorN([1.0, 0.0, 0.0])
let w = VectorN([0.0, 1.0, 0.0])
let u = VectorN([2.0, 0.0, 0.0])

// Check orthogonality
print(v.isOrthogonal(to: w))  // true
print(v.isOrthogonal(to: u))  // false

// Check parallelism
print(v.isParallel(to: u))    // true (same direction)
print(v.isParallel(to: w))    // false
```

---

## 5. MultivariateConstraint

Phase 2 introduced type-safe constraint specification for multivariate problems:

### Constraint Types

```swift
enum MultivariateConstraint<V: VectorSpace> {
    case equality(function: (V) -> V.Scalar, gradient: ((V) -> V)?)
    case inequality(function: (V) -> V.Scalar, gradient: ((V) -> V)?)
}
```

### Equality Constraints (h(x) = 0)

```swift
import BusinessMath

// Budget constraint: weights must sum to 1
let budgetConstraint: MultivariateConstraint<VectorN<Double>> = .equality { weights in
    weights.sum() - 1.0
}

// Usage with optimizer
let result = try optimizer.minimize(
    objective,
    from: initial,
    subjectTo: [budgetConstraint]
)
```

### Inequality Constraints (g(x) ≤ 0)

```swift
import BusinessMath

// Non-negativity: w[0] ≥ 0  →  -w[0] ≤ 0
let nonNegative: MultivariateConstraint<VectorN<Double>> = .inequality { w in
    -w[0]
}

// Maximum value: w[1] ≤ 0.5  →  w[1] - 0.5 ≤ 0
let maxValue: MultivariateConstraint<VectorN<Double>> = .inequality { w in
    w[1] - 0.5
}
```

### With Analytical Gradients

Provide gradients for better performance:

```swift
// Budget constraint: sum(w) - 1 = 0
let budgetConstraint: MultivariateConstraint<VectorN<Double>> = .equality(
    function: { w in w.sum() - 1.0 },
    gradient: { w in
        // ∇(sum(w) - 1) = [1, 1, ..., 1]
        VectorN(repeating: 1.0, count: w.dimension)
    }
)

// Non-negativity: -w[i] ≤ 0
func nonNegativityConstraint(index: Int, dimension: Int) -> MultivariateConstraint<VectorN<Double>> {
    .inequality(
        function: { w in -w[index] },
        gradient: { w in
            // ∇(-w[i]) = unit vector in direction i
            -VectorN<Double>.basisVector(dimension: dimension, index: index)
        }
    )
}
```

---

## 6. Scalar as VectorSpace

Even scalars (Double, Float) conform to VectorSpace:

```swift
// Double is a 1D vector space
let x: Double = 5.0

// VectorSpace operations
print(Double.zero)            // 0.0
print(x.norm)                 // 5.0 (absolute value)
print(x.dot(3.0))             // 15.0 (multiplication)
print(x.toArray())            // [5.0]
print(Double.dimension)       // 1

// This enables scalar optimization with same API
func optimize<V: VectorSpace>(_ f: (V) -> V.Scalar, ...) -> V

// Works for scalars
let scalarResult: Double = optimize(...)

// And vectors
let vectorResult: VectorN<Double> = optimize(...)
```

---

## 7. Use Cases and Examples

### Example 1: Portfolio Optimization

```swift
import BusinessMath

// 3 assets with weights
var weights = VectorN([0.3, 0.4, 0.3])

// Check if weights sum to 1
print(weights.sum)  // 1.0

// Expected returns
let returns = VectorN([0.10, 0.15, 0.12])

// Portfolio return (dot product)
let portfolioReturn = weights.dot(returns)
print(portfolioReturn)  // 0.127 (12.7%)

// Normalize weights if needed
if abs(weights.sum - 1.0) > 0.001 {
    weights = weights / weights.sum
}
```

### Example 2: Gradient Descent

```swift
import BusinessMath

// Minimize f(x, y) = x² + y²
func f(_ v: Vector2D<Double>) -> Double {
    v.x * v.x + v.y * v.y
}

// Gradient: ∇f = [2x, 2y]
func gradient(_ v: Vector2D<Double>) -> Vector2D<Double> {
    Vector2D(x: 2 * v.x, y: 2 * v.y)
}

// Gradient descent
var x = Vector2D(x: 10.0, y: 10.0)
let learningRate = 0.1

for _ in 0..<100 {
    let grad = gradient(x)
    x = x - learningRate * grad

    if grad.norm < 0.001 { break }
}

print("Minimum at: (\(x.x), \(x.y))")
print("Value: \(f(x))")
```

### Example 3: Distance Calculations

```swift
import BusinessMath

let cities = [
    "A": VectorN([0.0, 0.0]),
    "B": VectorN([3.0, 4.0]),
    "C": VectorN([6.0, 8.0])
]

// Euclidean distances
print("A to B:", cities["A"]!.distance(to: cities["B"]!))  // 5.0
print("B to C:", cities["B"]!.distance(to: cities["C"]!))  // 5.0
print("A to C:", cities["A"]!.distance(to: cities["C"]!))  // 10.0

// Manhattan distances (city blocks)
print("A to B (Manhattan):",
      cities["A"]!.manhattanDistance(to: cities["B"]!))  // 7.0
```

### Example 4: Feature Vectors

```swift
import BusinessMath

// Machine learning feature vectors
let features1 = VectorN([1.0, 2.0, 3.0, 4.0, 5.0])
let features2 = VectorN([5.0, 4.0, 3.0, 2.0, 1.0])

// Cosine similarity (for comparing features)
let similarity = features1.cosineSimilarity(with: features2)
print("Similarity: \(similarity)")  // 0.636...

// Normalize features (standardize)
let normalized1 = features1.normalized()
let normalized2 = features2.normalized()

print("Normalized norms:")
print(normalized1.norm)  // 1.0
print(normalized2.norm)  // 1.0
```

---

## 8. Performance Characteristics

### Vector Type Selection

| Type | Dimension | Performance | Use When |
|------|-----------|-------------|----------|
| `Double` | 1 | Fastest | Scalar problems |
| `Vector2D` | 2 | Very fast | 2D problems (known at compile-time) |
| `Vector3D` | 3 | Very fast | 3D problems (known at compile-time) |
| `VectorN` | N | Fast | Variable/high dimensions, unknown at compile-time |

### Optimization Tips

1. **Use fixed-dimension vectors when possible:**
   ```swift
   // ✅ Better for 2D problems
   let v: Vector2D<Double> = Vector2D(x: 1.0, y: 2.0)

   // ❌ Slower for known 2D problems
   let v: VectorN<Double> = VectorN([1.0, 2.0])
   ```

2. **Use squared norms for comparisons:**
   ```swift
   // ❌ Slower (unnecessary sqrt)
   if v.norm < 1.0 { ... }

   // ✅ Faster (no sqrt)
   if v.squaredNorm < 1.0 { ... }
   ```

3. **Provide analytical gradients when possible:**
   ```swift
   // ❌ Slower (numerical gradient)
   let constraint = MultivariateConstraint.equality { w in w.sum() - 1.0 }

   // ✅ Faster (analytical gradient)
   let constraint = MultivariateConstraint.equality(
       function: { w in w.sum() - 1.0 },
       gradient: { w in VectorN(repeating: 1.0, count: w.dimension) }
   )
   ```

---

## 9. Integration with Optimization

The VectorSpace protocol enables generic optimization algorithms:

```swift
// Phase 3: Multivariate optimizers work with any VectorSpace
public protocol MultivariateOptimizer {
    associatedtype V: VectorSpace

    func minimize(
        _ objective: @escaping (V) -> V.Scalar,
        from initial: V
    ) throws -> OptimizationResult<V>
}

// Example: Works for scalars, 2D, 3D, or N-dimensional vectors
let optimizer = AdamOptimizer<VectorN<Double>>()
let result = try optimizer.minimize(objectiveFunction, from: initial)
```

---

## 10. Best Practices

### 1. Choose the Right Vector Type

```swift
// ✅ Good: Known 2D problem
let point: Vector2D<Double> = Vector2D(x: 1.0, y: 2.0)

// ✅ Good: Variable dimension
let weights: VectorN<Double> = VectorN(portfolioWeights)

// ❌ Bad: Using VectorN for fixed 2D
let point: VectorN<Double> = VectorN([1.0, 2.0])  // Slower!
```

### 2. Check Dimensions

```swift
// ❌ Bad: Assuming dimensions match
let result = v.dot(w)

// ✅ Good: Verify dimensions
guard v.dimension == w.dimension else {
    throw OptimizationError.dimensionMismatch
}
let result = v.dot(w)
```

### 3. Use isFinite for Validation

```swift
// ✅ Good: Validate numeric stability
let gradient = computeGradient(at: x)
guard gradient.isFinite else {
    throw OptimizationError.numericalInstability
}
```

### 4. Leverage Functional Operations

```swift
// ❌ Verbose: Manual loop
var result = VectorN<Double>([])
for i in 0..<v.dimension {
    result = result.appending(v[i] * v[i])
}

// ✅ Concise: Functional style
let result = v.map { $0 * $0 }
```

---

## Summary

Phase 2's VectorSpace foundation provides:

✅ **Generic vector operations** via VectorSpace protocol
✅ **Multiple vector types** (Vector2D, Vector3D, VectorN) for different use cases
✅ **Rich operations** (norms, distances, projections, etc.)
✅ **Type-safe constraints** with MultivariateConstraint
✅ **Functional programming** support (map, filter, reduce, etc.)

This foundation enables:
- Generic optimization algorithms (Phase 3)
- Constrained optimization (Phase 4)
- Business optimization modules (Phase 5)

**Next Steps:**
- **Phase 3**: Learn multivariate optimization algorithms
- **Phase 4**: Add constraints to optimization
- **Phase 5**: Apply to business problems

---

**Phase 2: Complete** ✅
