# Phase 7: Performance Benchmarking - Complete Tutorial

**Created:** 2025-12-04
**Status:** Complete ‚úÖ

---

## Table of Contents

1. [Overview](#overview)
2. [Quick Start](#quick-start)
3. [Single Run Profiling](#single-run-profiling)
4. [Comparing Optimizers](#comparing-optimizers)
5. [Understanding Reports](#understanding-reports)
6. [Statistical Analysis](#statistical-analysis)
7. [Real-World Examples](#real-world-examples)
8. [Troubleshooting](#troubleshooting)
9. [Best Practices](#best-practices)

---

## Overview

Performance Benchmarking provides professional-grade tools to measure, compare, and analyze optimizer performance. Make evidence-based decisions about which optimizer to use for your specific problems.

### What Problems Does It Solve?

**Before Performance Benchmarking:**
- "Is this optimizer fast enough?"
- "Which algorithm should I use?"
- "Did my optimization change make things better or worse?"
- Manual timing is error-prone and inconsistent
- No way to compare algorithms systematically

**After Performance Benchmarking:**
- Precise timing with statistical analysis
- Side-by-side algorithm comparison
- Success rate tracking
- Professional reports
- Evidence-based decisions

### Key Features

- **High-Precision Timing**: Microsecond-accurate measurements
- **Statistical Analysis**: Mean, std dev, success rates
- **Multi-Optimizer Comparison**: Compare 2+ algorithms on same problem
- **Professional Reports**: Summary and detailed views
- **Winner Selection**: Automatically identifies best optimizer
- **Trial Averaging**: Multiple runs for reliability

---

## Quick Start

### Simplest Benchmark (3 lines)

```swift
import BusinessMath

let benchmark = PerformanceBenchmark<VectorN<Double>>()

let report = try benchmark.quickCompare(
    objective: { x in (x[0] - 1) * (x[0] - 1) + (x[1] - 2) * (x[1] - 2) },
    initialGuess: VectorN([0.0, 0.0]),
    trials: 10
)

print(report.summary())
```

**Output:**
```
=== Optimization Performance Comparison ===

Optimizer                  Avg Time  Iterations  Success Rate   Best Obj
---------------------------------------------------------------------------
‚Üí Default                    0.0012s        5.0       100.0%   0.000012
  Speed-Focused              0.0015s        4.2       100.0%   0.000023
  Accuracy-Focused           0.0010s        3.8       100.0%   0.000008

Winner: Default
  - Fastest average time: 0.0012s
  - Success rate: 100.0%
```

That's it! Instant comparison of three optimizer configurations.

---

## Single Run Profiling

### Basic Profiling

```swift
import BusinessMath

let benchmark = PerformanceBenchmark<VectorN<Double>>()
let optimizer = AdaptiveOptimizer<VectorN<Double>>()

let result = try benchmark.profileOptimizer(
    name: "My Optimizer",
    optimizer: optimizer,
    objective: { x in (x[0] - 1) * (x[0] - 1) + (x[1] - 2) * (x[1] - 2) },
    initialGuess: VectorN([0.0, 0.0])
)

print("Execution time: \(result.executionTime)s")
print("Iterations: \(result.iterations)")
print("Converged: \(result.converged)")
print("Solution: \(result.solution)")
print("Objective: \(result.objectiveValue)")
if let algorithm = result.algorithmName {
    print("Algorithm: \(algorithm)")
}
```

### Result Structure

```swift
public struct RunResult {
    public let solution: V                    // Solution found
    public let objectiveValue: Double         // Objective at solution
    public let executionTime: Double          // Time in seconds
    public let iterations: Int                // Number of iterations
    public let converged: Bool                // Whether converged
    public let algorithmName: String?         // Algorithm used
}
```

### Profiling with Constraints

```swift
let constraints: [MultivariateConstraint<VectorN<Double>>] = [
    .equality(function: { x in x.toArray().reduce(0, +) - 1.0 }, gradient: nil),
    .inequality(function: { x in -x[0] }, gradient: nil)
]

let result = try benchmark.profileOptimizer(
    name: "Constrained Optimizer",
    optimizer: AdaptiveOptimizer(),
    objective: portfolioVariance,
    initialGuess: equalWeights,
    constraints: constraints
)

print("Time with constraints: \(result.executionTime)s")
```

---

## Comparing Optimizers

### Compare Custom Configurations

```swift
import BusinessMath

let benchmark = PerformanceBenchmark<VectorN<Double>>()

let report = try benchmark.compareOptimizers(
    objective: rosenbrockFunction,
    optimizers: [
        ("Fast", AdaptiveOptimizer(preferSpeed: true)),
        ("Balanced", AdaptiveOptimizer()),
        ("Accurate", AdaptiveOptimizer(preferAccuracy: true)),
        ("High Tolerance", AdaptiveOptimizer(tolerance: 1e-4)),
        ("Low Tolerance", AdaptiveOptimizer(tolerance: 1e-8))
    ],
    initialGuess: VectorN([0.0, 0.0]),
    trials: 20  // Run each 20 times for reliability
)

print(report.summary())
```

### Quick Compare (Pre-configured)

```swift
// Convenience method: Compares Default, Speed-Focused, Accuracy-Focused
let report = try benchmark.quickCompare(
    objective: myObjective,
    initialGuess: initial,
    constraints: myConstraints,
    trials: 10
)

print(report.summary())
```

### Accessing Detailed Results

```swift
let report = try benchmark.compareOptimizers(...)

// Winner
print("Winner: \(report.winner.name)")
print("Average time: \(report.winner.avgTime)s")
print("Success rate: \(report.winner.successRate * 100)%")

// All optimizers
for result in report.results {
    print("\(result.name):")
    print("  Avg time: \(result.avgTime)s ¬± \(result.stdTime)s")
    print("  Avg iterations: \(result.avgIterations)")
    print("  Success: \(result.successRate * 100)%")
    print("  Best objective: \(result.bestObjectiveValue)")
    print("  Avg objective: \(result.avgObjectiveValue)")
}
```

---

## Understanding Reports

### Summary Report

The summary provides a quick overview:

```
=== Optimization Performance Comparison ===

Optimizer                  Avg Time  Iterations  Success Rate   Best Obj
---------------------------------------------------------------------------
‚Üí Winner                     0.0012s        5.0       100.0%   0.000012
  Runner-up                  0.0015s        4.2        95.0%   0.000023
  Slower                     0.0025s        8.1        90.0%   0.000008

Winner: Winner
  - Fastest average time: 0.0012s
  - Success rate: 100.0%
```

**Reading the Summary:**
- **‚Üí** marker indicates winner
- **Avg Time**: Average execution time across trials
- **Iterations**: Average iteration count
- **Success Rate**: Percentage of trials that converged
- **Best Obj**: Best objective value achieved across all trials

### Detailed Report

```swift
print(report.detailedReport())
```

**Output:**
```
=== Detailed Results ===

Default:
  Average time: 0.0012s (¬± 0.0002s)
  Average iterations: 5.0
  Success rate: 100.0%
  Average objective: 0.000015
  Best objective: 0.000012
  Runs:
    1: 0.0010s, 5 iter, obj=0.000012 ‚úì
    2: 0.0014s, 5 iter, obj=0.000018 ‚úì
    3: 0.0012s, 5 iter, obj=0.000015 ‚úì
    4: 0.0011s, 5 iter, obj=0.000013 ‚úì
    5: 0.0013s, 5 iter, obj=0.000016 ‚úì
```

**What You Get:**
- **Standard deviation** (¬±) shows timing variability
- **Run-by-run details** for first 5 trials
- **‚úì** indicates convergence
- **‚úó** would indicate failure

---

## Statistical Analysis

### OptimizerResult Structure

```swift
public struct OptimizerResult {
    public let name: String                   // Optimizer name
    public let avgTime: Double                // Average execution time
    public let stdTime: Double                // Standard deviation of time
    public let avgIterations: Double          // Average iterations
    public let successRate: Double            // Proportion that converged (0-1)
    public let avgObjectiveValue: Double      // Average objective (successful)
    public let bestObjectiveValue: Double     // Best objective achieved
    public let runs: [RunResult]              // All individual runs
}
```

### Understanding Statistics

#### Average Time

```swift
// Arithmetic mean of all trial times
let avgTime = trials.map(\.executionTime).reduce(0, +) / Double(trials.count)
```

**Interpretation:**
- Expected time for a single run
- Lower is better
- Consider std dev for variability

#### Standard Deviation

```swift
let mean = avgTime
let squaredDiffs = trials.map { pow($0.executionTime - mean, 2) }
let variance = squaredDiffs.reduce(0, +) / Double(trials.count)
let stdDev = sqrt(variance)
```

**Interpretation:**
- **Low std dev** (<10% of mean): Consistent performance
- **High std dev** (>20% of mean): Variable performance, run more trials

#### Success Rate

```swift
let successRate = Double(convergedRuns.count) / Double(totalRuns)
```

**Interpretation:**
- **100%**: Algorithm always converges (excellent)
- **>80%**: Reliable algorithm
- **<50%**: Problem or algorithm mismatch

#### Winner Selection

```swift
// Winner: Fastest with >50% success rate
let viable = allResults.filter { $0.successRate > 0.5 }
let winner = viable.min(by: { $0.avgTime < $1.avgTime })
```

**Why this rule:**
- Speed doesn't matter if algorithm fails
- 50% threshold ensures reliability
- Among reliable algorithms, fastest wins

---

## Real-World Examples

### Example 1: Algorithm Selection for Production

You're deploying an optimizer in production and need to choose between configurations.

```swift
import BusinessMath

let benchmark = PerformanceBenchmark<VectorN<Double>>()

// Your typical production problem
let typicalObjective: (VectorN<Double>) -> Double = { x in
    // Your actual production objective function
    let costs = x.toArray().reduce(0.0) { $0 + $1 * $1 }
    let penalties = max(0, x.toArray().reduce(0, +) - 10.0)
    return costs + 1000 * penalties
}

let typicalInitial = VectorN([1.0, 2.0, 3.0, 4.0])

let report = try benchmark.compareOptimizers(
    objective: typicalObjective,
    optimizers: [
        ("Default", AdaptiveOptimizer()),
        ("Fast", AdaptiveOptimizer(preferSpeed: true, tolerance: 1e-4)),
        ("Precise", AdaptiveOptimizer(preferAccuracy: true, tolerance: 1e-8))
    ],
    initialGuess: typicalInitial,
    trials: 50  // Many trials for production confidence
)

print(report.summary())

// Make decision
let winner = report.winner
print("\nüéØ Recommendation for production:")
print("Use: \(winner.name)")
print("Expected time: \(winner.avgTime)s per optimization")
print("Reliability: \(winner.successRate * 100)% success rate")
print("Quality: \(winner.avgObjectiveValue) average objective")

// Validate meets SLA
assert(winner.avgTime < 0.1, "Must complete in <100ms")
assert(winner.successRate > 0.95, "Must have >95% success rate")
```

### Example 2: Regression Testing

You made changes to an optimizer and want to ensure performance didn't degrade.

```swift
import BusinessMath

let benchmark = PerformanceBenchmark<VectorN<Double>>()

// Standard benchmark problem
let standardBenchmark: (VectorN<Double>) -> Double = { x in
    (x[0] - 1) * (x[0] - 1) + (x[1] - 2) * (x[1] - 2)
}

// Baseline (previous version)
let baselineOptimizer = AdaptiveOptimizer<VectorN<Double>>()

let baseline = try benchmark.profileOptimizer(
    name: "Baseline",
    optimizer: baselineOptimizer,
    objective: standardBenchmark,
    initialGuess: VectorN([0.0, 0.0])
)

// New version (with your changes)
let newOptimizer = AdaptiveOptimizer<VectorN<Double>>(
    // Your modifications here
    tolerance: 1e-7
)

let newVersion = try benchmark.profileOptimizer(
    name: "New Version",
    optimizer: newOptimizer,
    objective: standardBenchmark,
    initialGuess: VectorN([0.0, 0.0])
)

// Compare
print("Baseline: \(baseline.executionTime)s, \(baseline.iterations) iter")
print("New: \(newVersion.executionTime)s, \(newVersion.iterations) iter")

let speedup = baseline.executionTime / newVersion.executionTime
if speedup > 1.0 {
    print("‚úÖ New version is \(speedup)x faster!")
} else {
    print("‚ö†Ô∏è New version is \(1/speedup)x slower")
}

// Automated regression test
assert(newVersion.converged, "New version must converge")
assert(newVersion.executionTime < baseline.executionTime * 1.2,
       "New version must not be >20% slower")
```

### Example 3: Hyperparameter Tuning

Find the best tolerance setting for your problem.

```swift
import BusinessMath

let benchmark = PerformanceBenchmark<VectorN<Double>>()

let tolerances = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8]

let report = try benchmark.compareOptimizers(
    objective: complexObjective,
    optimizers: tolerances.map { tol in
        ("tol=\(tol)", AdaptiveOptimizer(tolerance: tol))
    },
    initialGuess: initialGuess,
    trials: 20
)

print(report.summary())

// Find sweet spot (balance speed vs accuracy)
let winner = report.winner
print("\nüéØ Optimal tolerance: \(winner.name)")
print("  Speed: \(winner.avgTime)s")
print("  Accuracy: \(winner.avgObjectiveValue)")
print("  Reliability: \(winner.successRate * 100)%")
```

### Example 4: Problem Difficulty Assessment

Understand how difficult your optimization problem is.

```swift
import BusinessMath

let benchmark = PerformanceBenchmark<VectorN<Double>>()

let problems: [(String, (VectorN<Double>) -> Double)] = [
    ("Easy Quadratic", { x in x[0]*x[0] + x[1]*x[1] }),
    ("Medium Rosenbrock", rosenbrockFunction),
    ("Hard Non-Convex", complexNonConvexFunction)
]

for (name, objective) in problems {
    let result = try benchmark.profileOptimizer(
        name: name,
        optimizer: AdaptiveOptimizer(),
        objective: objective,
        initialGuess: VectorN([0.0, 0.0])
    )

    print("\(name):")
    print("  Time: \(result.executionTime)s")
    print("  Iterations: \(result.iterations)")
    print("  Converged: \(result.converged ? "‚úì" : "‚úó")")
    print("  Objective: \(result.objectiveValue)")
}
```

### Example 5: Comparing Against Manual Implementation

Verify that Adaptive Optimizer performs well against hand-tuned code.

```swift
import BusinessMath

let benchmark = PerformanceBenchmark<VectorN<Double>>()

// Your hand-tuned manual optimizer
class ManualOptimizer {
    func optimize(objective: @escaping (VectorN<Double>) -> Double,
                  initialGuess: VectorN<Double>) throws
                  -> (solution: VectorN<Double>, value: Double, iterations: Int, converged: Bool) {
        // Your carefully tuned implementation
        // ...
        return (solution, value, iterations, converged)
    }
}

// Wrap manual optimizer to match interface
let manualResult = try benchmark.profileOptimizer(
    name: "Manual (Hand-Tuned)",
    optimizer: AdaptiveOptimizer(),  // Actually call your ManualOptimizer
    objective: yourObjective,
    initialGuess: initial
)

let adaptiveResult = try benchmark.profileOptimizer(
    name: "Adaptive (Automatic)",
    optimizer: AdaptiveOptimizer(),
    objective: yourObjective,
    initialGuess: initial
)

print("Manual:   \(manualResult.executionTime)s, obj=\(manualResult.objectiveValue)")
print("Adaptive: \(adaptiveResult.executionTime)s, obj=\(adaptiveResult.objectiveValue)")

if adaptiveResult.executionTime < manualResult.executionTime * 1.1 &&
   adaptiveResult.objectiveValue <= manualResult.objectiveValue * 1.01 {
    print("‚úÖ Adaptive optimizer matches hand-tuned performance!")
}
```

---

## Troubleshooting

### Problem: Inconsistent Timing Results

**Symptoms:**
- Large standard deviation
- Results vary widely between runs
- Timing seems random

**Solutions:**

1. **Run More Trials**
   ```swift
   let report = try benchmark.compareOptimizers(
       ...,
       trials: 50  // Increase from 10 to 50+
   )
   ```

2. **Warm Up First**
   ```swift
   // Run once to warm up JIT/caches
   _ = try optimizer.optimize(objective: f, initialGuess: x0)

   // Now benchmark
   let result = try benchmark.profileOptimizer(...)
   ```

3. **Check System Load**
   - Close other applications
   - Disable background processes
   - Run on consistent hardware

### Problem: All Optimizers Show 0% Success Rate

**Symptoms:**
- `successRate == 0.0` for all optimizers
- No algorithms converging

**Likely Causes:**

1. **Problem is Too Difficult**
   ```swift
   // Relax convergence criteria
   let easier = AdaptiveOptimizer<VectorN<Double>>(
       tolerance: 1e-4,  // Relax from 1e-6
       maxIterations: 5000  // Increase from 1000
   )
   ```

2. **Bad Initial Guess**
   ```swift
   // Try multiple starting points
   let starts = [
       VectorN([0.0, 0.0]),
       VectorN([1.0, 1.0]),
       VectorN([0.5, 0.5])
   ]

   for initial in starts {
       let result = try benchmark.profileOptimizer(
           name: "Start: \(initial)",
           optimizer: optimizer,
           objective: objective,
           initialGuess: initial
       )
       if result.converged {
           print("‚úì Converged from \(initial)")
       }
   }
   ```

3. **Problem Formulation Issue**
   - Check objective function is correct
   - Verify constraints are feasible
   - Test on simpler problem first

### Problem: Winner Has Low Success Rate

**Symptoms:**
- Winner selected despite <80% success rate
- All optimizers struggling

**What This Means:**
- Winner is "best of bad options"
- Problem needs attention

**Solutions:**

```swift
let winner = report.winner

if winner.successRate < 0.8 {
    print("‚ö†Ô∏è Warning: Best optimizer only \(winner.successRate * 100)% reliable")
    print("Consider:")
    print("- Increasing maxIterations")
    print("- Relaxing tolerance")
    print("- Improving initial guess")
    print("- Simplifying problem")
}
```

### Problem: Timing Seems Too Fast or Too Slow

**Symptoms:**
- Times in nanoseconds (too fast)
- Times in minutes (too slow)

**Explanations:**

**Too Fast (<0.001s):**
- Problem is very simple
- Optimizer converging in 1-2 iterations
- This is fine! Simple problems are fast.

**Too Slow (>10s):**
- Large problem (many variables)
- Difficult objective function
- Many iterations

```swift
let result = try benchmark.profileOptimizer(...)

print("Time: \(result.executionTime)s")
print("Iterations: \(result.iterations)")
print("Time per iteration: \(result.executionTime / Double(result.iterations))s")

// If time per iteration is reasonable but total is high:
// ‚Üí Problem needs many iterations (normal)

// If time per iteration is slow:
// ‚Üí Objective function is expensive (optimize it)
```

---

## Best Practices

### 1. Run Sufficient Trials

```swift
// ‚ùå BAD: Single trial (unreliable)
let report = try benchmark.compareOptimizers(..., trials: 1)

// ‚úÖ GOOD: Multiple trials for statistics
let report = try benchmark.compareOptimizers(..., trials: 20)

// ‚úÖ BEST: Many trials for production decisions
let report = try benchmark.compareOptimizers(..., trials: 50)
```

**Guidelines:**
- **Development**: 10 trials
- **Testing**: 20 trials
- **Production decisions**: 50+ trials
- **Research/publication**: 100+ trials

### 2. Use Representative Problems

```swift
// ‚ùå BAD: Benchmarking toy problem
let toy = { x in x[0] * x[0] }

// ‚úÖ GOOD: Benchmark actual production problem
let production = { x in
    complexProductionCostFunction(x)
}

let report = try benchmark.compareOptimizers(
    objective: production,  // Your real objective
    initialGuess: typicalInitial,  // Your typical starting point
    constraints: actualConstraints,  // Your actual constraints
    trials: 50
)
```

### 3. Report Full Context

```swift
let report = try benchmark.compareOptimizers(...)

// Save complete benchmark results
let context = """
Benchmark Results
=================
Date: \(Date())
Problem: Portfolio optimization (3 assets)
Dimensions: \(initialGuess.toArray().count)
Constraints: \(constraints.count)
Trials: 50

\(report.detailedReport())
"""

try context.write(to: URL(fileURLWithPath: "benchmark_results.txt"),
                   atomically: true,
                   encoding: .utf8)
```

### 4. Validate Winner Makes Sense

```swift
let winner = report.winner

// Sanity checks
assert(winner.successRate > 0.5, "Winner must be reliable")
assert(winner.avgTime > 0, "Time must be positive")
assert(winner.avgIterations > 0, "Must take iterations")

// Business validation
if winner.avgTime > maxAcceptableTime {
    print("‚ö†Ô∏è Winner is too slow for production")
    print("Consider: relaxing tolerance, preferring speed")
}
```

### 5. Consider Speed vs Accuracy Trade-offs

```swift
// For different use cases
let scenarios = [
    ("Interactive (Speed)", AdaptiveOptimizer(preferSpeed: true, tolerance: 1e-4)),
    ("Production (Balanced)", AdaptiveOptimizer()),
    ("Critical (Accuracy)", AdaptiveOptimizer(preferAccuracy: true, tolerance: 1e-8))
]

let report = try benchmark.compareOptimizers(
    objective: objective,
    optimizers: scenarios.map { ($0.0, $0.1) },
    initialGuess: initial,
    trials: 30
)

print(report.summary())

// Choose based on use case
// - Interactive: Use fastest
// - Production: Use balanced
// - Critical: Use most accurate
```

### 6. Automate Benchmarking in CI/CD

```swift
// In your test suite
func testPerformanceRegression() throws {
    let benchmark = PerformanceBenchmark<VectorN<Double>>()

    let result = try benchmark.profileOptimizer(
        name: "Standard Benchmark",
        optimizer: AdaptiveOptimizer(),
        objective: standardObjective,
        initialGuess: standardInitial
    )

    // Assert performance hasn't regressed
    XCTAssertTrue(result.converged, "Must converge")
    XCTAssertLessThan(result.executionTime, 0.1, "Must complete in <100ms")
    XCTAssertLessThanOrEqual(result.iterations, 50, "Must not need >50 iterations")
}
```

### 7. Document Benchmarking Methodology

```swift
/*
 Benchmarking Methodology
 ========================

 Objective: Portfolio variance minimization (3 assets)
 Initial Guess: Equal weights [1/3, 1/3, 1/3]
 Constraints: Budget constraint (sum=1) + non-negativity
 Trials: 50 per optimizer
 Hardware: MacBook Pro M1, 16GB RAM
 Date: 2025-12-04

 Results:
 - Winner: Adaptive (Default)
 - Avg Time: 0.0045s
 - Success Rate: 100%
 */

let report = try benchmark.compareOptimizers(...)
```

---

## Summary

### Key Takeaways

1. **Precise Measurement**: High-precision timing with statistical analysis
2. **Easy Comparison**: Side-by-side optimizer evaluation
3. **Evidence-Based**: Make decisions backed by data
4. **Production Ready**: Regression testing and validation
5. **Professional Reports**: Publication-quality output

### When to Use Performance Benchmarking

**‚úÖ USE for:**
- Choosing optimizer for production
- Validating performance improvements
- Regression testing
- Hyperparameter tuning
- Research and publications
- Understanding problem difficulty

**‚ö†Ô∏è DON'T USE when:**
- Just need to optimize once (use AdaptiveOptimizer directly)
- Timing precision not important
- No need to compare alternatives

### Quick Reference

```swift
// Single profiling
let result = try benchmark.profileOptimizer(
    name: "My Optimizer",
    optimizer: AdaptiveOptimizer(),
    objective: objective,
    initialGuess: initial
)

// Quick comparison
let report = try benchmark.quickCompare(
    objective: objective,
    initialGuess: initial,
    trials: 20
)

// Custom comparison
let report = try benchmark.compareOptimizers(
    objective: objective,
    optimizers: [...],
    initialGuess: initial,
    trials: 50
)

// Results
print(report.summary())
print(report.detailedReport())
print("Winner: \(report.winner.name)")
```

---

## Additional Resources

- **API Reference**: [PerformanceBenchmark.swift](../Sources/BusinessMath/Optimization/PerformanceBenchmark.swift)
- **Tests**: [PerformanceBenchmarkTests.swift](../Tests/BusinessMathTests/Performance%20Tests/PerformanceBenchmarkTests.swift)
- **Complete Documentation**: [PHASE_7_FEATURE_3_COMPLETE.md](PHASE_7_FEATURE_3_COMPLETE.md)
- **Framework Index**: [OPTIMIZATION_FRAMEWORK_INDEX.md](OPTIMIZATION_FRAMEWORK_INDEX.md)

---

**Happy Benchmarking! üìä**

*With Performance Benchmarking, optimization decisions are now backed by data, not guesswork.*
