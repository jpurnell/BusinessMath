//
//  drdSum.swift
//  BusinessMath
//
//  Numerically stable summation using Kahan's compensated summation algorithm.
//  Prevents catastrophic cancellation and maintains precision for large datasets.
//

import Foundation
import Numerics

/// Computes the sum of an array using Kahan's compensated summation algorithm.
///
/// This algorithm maintains numerical stability by tracking and compensating for
/// floating-point rounding errors that accumulate during summation. It is especially
/// important for large datasets where naive `reduce(0, +)` can overflow or lose precision.
///
/// ## Algorithm
///
/// Kahan summation works by maintaining a running compensation value that tracks
/// the low-order bits lost to rounding. On each iteration:
/// 1. Add the compensation to the current value
/// 2. Perform the addition
/// 3. Calculate the new compensation (what was lost in rounding)
///
/// ## Performance
///
/// - Time complexity: O(n)
/// - Space complexity: O(1)
/// - Overhead: ~4Ã— more operations than naive sum, but prevents overflow
///
/// ## Example
///
/// ```swift
/// // Naive sum overflows with large datasets
/// let values = Array(repeating: 1e308, count: 100)
/// let naiveSum = values.reduce(0.0, +)  // Infinity!
///
/// // Kahan sum handles it correctly
/// let stableSum = kahanSum(values)  // Finite value
/// ```
///
/// - Parameter values: Array of values to sum
/// - Returns: Numerically stable sum of all values
///
/// - Note: This function is critical for computing statistics on large datasets
///   where overflow would otherwise occur (e.g., 50,000+ samples).
public func kahanSum<T: Real>(_ values: [T]) -> T {
    guard !values.isEmpty else { return T(0) }

    var sum = T(0)
    var compensation = T(0)  // Tracks low-order bits lost to rounding

    for value in values {
        // Compensate for previous rounding errors
        let y = value - compensation

        // Add to sum (may lose low-order bits)
        let temp = sum + y

        // Calculate what was lost in the addition
        // (temp - sum) recovers the high-order part of y
        // Subtracting that from y gives the low-order part that was lost
        compensation = (temp - sum) - y

        sum = temp
    }

    return sum
}
